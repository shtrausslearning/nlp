{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"string = 'Today is a good day for taking a walk'\ntokens = string.split()\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T19:56:28.535178Z","iopub.execute_input":"2023-03-25T19:56:28.535740Z","iopub.status.idle":"2023-03-25T19:56:28.541829Z","shell.execute_reply.started":"2023-03-25T19:56:28.535708Z","shell.execute_reply":"2023-03-25T19:56:28.540494Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"['Today', 'is', 'a', 'good', 'day', 'for', 'taking', 'a', 'walk']\n","output_type":"stream"}]},{"cell_type":"code","source":"token_set = set(tokens) # create all unique tokens\nword2id = {word:idx for idx,word in enumerate(token_set)} # give unique identifier to each unique token\nid2word = {idx:word for idx,word in enumerate(token_set)}\nwindow = 2        # context window size\nembeddings = 10  # number of embeddings to be used for representation\nepochs = 100     # number of training iterations\nlr = 0.001       # learning rate for CBOW \n\nprint(token_set) # vocabulary\nvocab_size = len(token_set)  # size of vocabulary","metadata":{"execution":{"iopub.status.busy":"2023-03-25T20:00:56.823561Z","iopub.execute_input":"2023-03-25T20:00:56.823927Z","iopub.status.idle":"2023-03-25T20:00:56.830149Z","shell.execute_reply.started":"2023-03-25T20:00:56.823894Z","shell.execute_reply":"2023-03-25T20:00:56.829328Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"{'Today', 'is', 'walk', 'taking', 'good', 'a', 'day', 'for'}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef context_vector(tokens:list):\n    # list of values for each token\n    val_context = [word2id[word] for word in tokens] \n    return val_context\n    \n    \ncontext_pairs = []\n\n# loop through all possible cases \nfor i in range(window,len(tokens) - window):\n    \n    context = []\n    \n    # words to the left\n    for j in range(-window,0):\n        context.append(tokens[i+j])\n    \n    # words to the right\n    for j in range(1,window+1):\n        context.append(tokens[i+j])\n        \n    context_pairs.append((context,tokens[i]))\n    \n# show all context pairs in document\nprint('context, target pairs\\n')\nfor context in context_pairs:\n    print(context)\n    \n# sample tensor conversion\nprint('\\nfor pytorch; context, target word tensors\\n')\nfor context,target in context_pairs:\n    X = torch.tensor(context_vector(context))\n    y = torch.tensor(word2id[target])\n    print(X,y)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T20:00:58.305247Z","iopub.execute_input":"2023-03-25T20:00:58.305970Z","iopub.status.idle":"2023-03-25T20:00:58.316803Z","shell.execute_reply.started":"2023-03-25T20:00:58.305935Z","shell.execute_reply":"2023-03-25T20:00:58.315794Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"context, target pairs\n\n(['Today', 'is', 'good', 'day'], 'a')\n(['is', 'a', 'day', 'for'], 'good')\n(['a', 'good', 'for', 'taking'], 'day')\n(['good', 'day', 'taking', 'a'], 'for')\n(['day', 'for', 'a', 'walk'], 'taking')\n\nfor pytorch; context, target word tensors\n\ntensor([0, 1, 4, 6]) tensor(5)\ntensor([1, 5, 6, 7]) tensor(4)\ntensor([5, 4, 7, 3]) tensor(6)\ntensor([4, 6, 3, 5]) tensor(7)\ntensor([6, 7, 5, 2]) tensor(3)\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.optim import Adam\n\nclass CBOW(torch.nn.Module):\n    \n    def __init__(self,vocab_size,embed_dim):\n        super(CBOW,self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size,embed_dim)\n        self.linear = nn.Linear(embed_dim,vocab_size)\n#         self.active = nn.ReLU()\n        self.active = nn.LogSoftmax(dim=-1)\n        \n    def forward(self,x):\n        x = sum(self.embedding(x)).view(1,-1)\n        x = self.linear(x)\n        x = self.active(x)\n        return x\n    \n    def word_embedding(self, x):\n        word = torch.tensor([word2id[x]])\n        return self.embedding(word).view(1,-1)\n    \n\nmodel = CBOW(vocab_size,embeddings)    \ncriterion = nn.NLLLoss()\noptimiser = Adam(model.parameters(),lr=lr)","metadata":{"execution":{"iopub.status.busy":"2023-03-25T20:01:05.493739Z","iopub.execute_input":"2023-03-25T20:01:05.494082Z","iopub.status.idle":"2023-03-25T20:01:05.503530Z","shell.execute_reply.started":"2023-03-25T20:01:05.494053Z","shell.execute_reply":"2023-03-25T20:01:05.501605Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"model.word_embedding('day')  # randomly initialised weights","metadata":{"execution":{"iopub.status.busy":"2023-03-25T20:01:07.040197Z","iopub.execute_input":"2023-03-25T20:01:07.040630Z","iopub.status.idle":"2023-03-25T20:01:07.048461Z","shell.execute_reply.started":"2023-03-25T20:01:07.040595Z","shell.execute_reply":"2023-03-25T20:01:07.047634Z"},"trusted":true},"execution_count":104,"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.4653, -0.5790, -0.0121,  0.7312,  0.5235, -1.0668, -1.2971,  1.1165,\n          1.2788, -1.3157]], grad_fn=<ViewBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"# training loop\n\nlst_loss = []\nfor epoch in range(epochs):\n    \n    loss = 0.0\n    for context,target in context_pairs:\n        \n        X = torch.tensor(context_vector(context))\n        y = torch.tensor([word2id[target]])        \n\n        y_pred = model(X)\n        loss += criterion(y_pred,y)\n        \n    optimiser.zero_grad()\n    loss.backward()\n    optimiser.step()\n    lst_loss.append(float(loss.detach().numpy()))\n        \nprint(lst_loss[-1])","metadata":{"execution":{"iopub.status.busy":"2023-03-25T20:01:08.527255Z","iopub.execute_input":"2023-03-25T20:01:08.528553Z","iopub.status.idle":"2023-03-25T20:01:08.642571Z","shell.execute_reply.started":"2023-03-25T20:01:08.528488Z","shell.execute_reply":"2023-03-25T20:01:08.641598Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"6.324942111968994\n","output_type":"stream"}]},{"cell_type":"code","source":"''' Test out our CBOW model '''\n\n# tokenised text\ntemp = ['good','day','taking','a']\n\n# convert to tensor\ncont_vector = torch.tensor(context_vector(temp)) \n\n# prediction\ny_pred = model(cont_vector)\nprint(y_pred)\npred_index = torch.argmax(y_pred[0]) # get largest argument\nprint(f'prediction: ',id2word[pred_index.item()])","metadata":{"execution":{"iopub.status.busy":"2023-03-25T20:01:11.028381Z","iopub.execute_input":"2023-03-25T20:01:11.029419Z","iopub.status.idle":"2023-03-25T20:01:11.037919Z","shell.execute_reply.started":"2023-03-25T20:01:11.029384Z","shell.execute_reply":"2023-03-25T20:01:11.036690Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"tensor([[-3.1887, -2.5548, -2.2363, -2.6371, -2.8696, -2.0258, -1.5787, -1.1783]],\n       grad_fn=<LogSoftmaxBackward0>)\nprediction:  for\n","output_type":"stream"}]},{"cell_type":"code","source":"model.word_embedding('day')  # embedding for word day after training","metadata":{"execution":{"iopub.status.busy":"2023-03-25T20:01:18.379992Z","iopub.execute_input":"2023-03-25T20:01:18.380330Z","iopub.status.idle":"2023-03-25T20:01:18.389497Z","shell.execute_reply.started":"2023-03-25T20:01:18.380301Z","shell.execute_reply":"2023-03-25T20:01:18.388331Z"},"trusted":true},"execution_count":107,"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.5619, -0.5008, -0.0986,  0.6555,  0.5892, -1.1915, -1.2203,  1.0517,\n          1.1961, -1.4373]], grad_fn=<ViewBackward0>)"},"metadata":{}}]}]}