{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Single Document\n\nTokenisation of a single document in `string`format","metadata":{}},{"cell_type":"code","source":"# Splits words by space (split=” “).\n# Filters out punctuation (filters=’!”#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n’).\n# Converts text to lowercase (lower=True).\n\nfrom keras.preprocessing.text import text_to_word_sequence\n\n# Corpus\ncorpus = ['With Leonard, Howard, Raj, and Amy accomplishing so much on their respective projects, Sheldon is forced to admit he has nothing important upon which to work.', \n          'He makes Amy leave the apartment for a few days so he can focus, but cannot come up with any ideas and calls his mother as a distraction.',\n          'Leonard and Amy have fun recreating experiments from when they were growing up, boring Penny, so she eats with Sheldon as he mulls over his scientific studies.',\n          'Penny helps him realize that his study of dark matter is his rebound science from string theory, which Sheldon admits he never truly disregarded, but explaining string theory to her inspires Sheldon, helping him discover a potential breakthrough in the field.',\n          'Meanwhile, Howard is too busy with his family to be in the band with Raj, so Raj brings in Bert.',\n          'But when Howard annoys Bernadette by writing an astronaut-themed musical while she is on bed rest, she makes him rejoin the band.',\n          \"The three are poorly received at a Bar mitzvah after singing Bert's original song about the boulder from Raiders of the Lost Ark.\"]\n\n# Just tokenisation\nwords = text_to_word_sequence(corpus[0])\nprint(words)\nprint(len(words),'\\n')\n\n# Estimate size of vocabulary\nunique_words = set(text_to_word_sequence(corpus[0]))\nprint(unique_words)\n\nvocab_size = len(words)\nprint(vocab_size)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-18T06:29:46.845517Z","iopub.execute_input":"2022-12-18T06:29:46.846024Z","iopub.status.idle":"2022-12-18T06:29:46.857152Z","shell.execute_reply.started":"2022-12-18T06:29:46.845985Z","shell.execute_reply":"2022-12-18T06:29:46.855398Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"['with', 'leonard', 'howard', 'raj', 'and', 'amy', 'accomplishing', 'so', 'much', 'on', 'their', 'respective', 'projects', 'sheldon', 'is', 'forced', 'to', 'admit', 'he', 'has', 'nothing', 'important', 'upon', 'which', 'to', 'work']\n26 \n\n{'respective', 'to', 'important', 'howard', 'their', 'has', 'leonard', 'work', 'on', 'accomplishing', 'amy', 'projects', 'forced', 'nothing', 'admit', 'is', 'with', 'much', 'so', 'and', 'upon', 'which', 'sheldon', 'he', 'raj'}\n26\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.preprocessing.text import one_hot\n\n# integer encode the document\nresult = one_hot(corpus[0], round(vocab_size*1.3))\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T06:29:49.651374Z","iopub.execute_input":"2022-12-18T06:29:49.651779Z","iopub.status.idle":"2022-12-18T06:29:49.659160Z","shell.execute_reply.started":"2022-12-18T06:29:49.651748Z","shell.execute_reply":"2022-12-18T06:29:49.657622Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"[33, 2, 12, 24, 16, 6, 18, 13, 6, 31, 15, 3, 21, 32, 33, 22, 31, 9, 27, 10, 14, 22, 33, 28, 31, 15]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Multiple Documents\n\nStandard Word Level Tokenisers for multiple documents in a list corpus","metadata":{}},{"cell_type":"code","source":"# Corpus\ncorpus = ['With Leonard, Howard, Raj, and Amy accomplishing so much on their respective projects, Sheldon is forced to admit he has nothing important upon which to work.', \n          'He makes Amy leave the apartment for a few days so he can focus, but cannot come up with any ideas and calls his mother as a distraction.',\n          'Leonard and Amy have fun recreating experiments from when they were growing up, boring Penny, so she eats with Sheldon as he mulls over his scientific studies.',\n          'Penny helps him realize that his study of dark matter is his rebound science from string theory, which Sheldon admits he never truly disregarded, but explaining string theory to her inspires Sheldon, helping him discover a potential breakthrough in the field.',\n          'Meanwhile, Howard is too busy with his family to be in the band with Raj, so Raj brings in Bert.',\n          'But when Howard annoys Bernadette by writing an astronaut-themed musical while she is on bed rest, she makes him rejoin the band.',\n          \"The three are poorly received at a Bar mitzvah after singing Bert's original song about the boulder from Raiders of the Lost Ark.\"]\n\nlabels = np.array([0,0,1,1,0,1,1])","metadata":{"execution":{"iopub.status.busy":"2022-12-18T08:21:39.351830Z","iopub.execute_input":"2022-12-18T08:21:39.352271Z","iopub.status.idle":"2022-12-18T08:21:39.359648Z","shell.execute_reply.started":"2022-12-18T08:21:39.352235Z","shell.execute_reply":"2022-12-18T08:21:39.358362Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"markdown","source":"### Tokenise Documents (Word Level)\n\ntokeniser in `preprocessing.text` \n","metadata":{}},{"cell_type":"code","source":"'''\n\nGENERATE TOKENS\n\n'''\n\nfrom keras.preprocessing.text import Tokenizer\n\nt = Tokenizer(oov_token='') # create the tokenizer\nt.fit_on_texts(corpus) # fit the tokenizer on the documents\n\n# Dictionary\nprint(t.word_index,'\\n')\n\n\n'''\n\nGENERATE FEATURE MATRIX\ntokenise.text_to_matrix\n\n'''\n\n# ‘binary‘: Whether or not each word is present in the document. This is the default. OHE\n# ‘count‘: The count of each word in the document. BOW\n# ‘tfidf‘: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n# ‘freq‘: The frequency of each word as a ratio of words within each document.\n\nencoded_docs = t.texts_to_matrix(corpus, mode='binary')\nprint(encoded_docs.shape)\nencoded_docs[0] # one of the documents \n\n'''\n\nPADDING\nConvert all encoded documents to the same length\n\n'''\n\n# Check length of feature matrix\n\nimport numpy as np\n\nencoded_docs = t.texts_to_sequences(corpus)\n\nprint('tokenised document data:')\nfor ii,i in enumerate(encoded_docs):\n    print(ii,f'document length: {len(i)}')\n\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_length = 4\n# Pad documents to a max length of 4 words\n\npadded_docs = pad_sequences(encoded_docs, \n                            maxlen=68, \n                            padding='post')\nprint(f'\\npadded documents:\\n {padded_docs}\\n')\nprint(padded_docs.shape)","metadata":{"execution":{"iopub.status.busy":"2022-12-18T08:17:31.562384Z","iopub.execute_input":"2022-12-18T08:17:31.563428Z","iopub.status.idle":"2022-12-18T08:17:31.578072Z","shell.execute_reply.started":"2022-12-18T08:17:31.563388Z","shell.execute_reply":"2022-12-18T08:17:31.576397Z"},"trusted":true},"execution_count":147,"outputs":[{"name":"stdout","text":"{'': 1, 'the': 2, 'with': 3, 'he': 4, 'his': 5, 'so': 6, 'sheldon': 7, 'is': 8, 'to': 9, 'a': 10, 'howard': 11, 'raj': 12, 'and': 13, 'amy': 14, 'but': 15, 'from': 16, 'she': 17, 'him': 18, 'in': 19, 'leonard': 20, 'on': 21, 'which': 22, 'makes': 23, 'up': 24, 'as': 25, 'when': 26, 'penny': 27, 'of': 28, 'string': 29, 'theory': 30, 'band': 31, 'accomplishing': 32, 'much': 33, 'their': 34, 'respective': 35, 'projects': 36, 'forced': 37, 'admit': 38, 'has': 39, 'nothing': 40, 'important': 41, 'upon': 42, 'work': 43, 'leave': 44, 'apartment': 45, 'for': 46, 'few': 47, 'days': 48, 'can': 49, 'focus': 50, 'cannot': 51, 'come': 52, 'any': 53, 'ideas': 54, 'calls': 55, 'mother': 56, 'distraction': 57, 'have': 58, 'fun': 59, 'recreating': 60, 'experiments': 61, 'they': 62, 'were': 63, 'growing': 64, 'boring': 65, 'eats': 66, 'mulls': 67, 'over': 68, 'scientific': 69, 'studies': 70, 'helps': 71, 'realize': 72, 'that': 73, 'study': 74, 'dark': 75, 'matter': 76, 'rebound': 77, 'science': 78, 'admits': 79, 'never': 80, 'truly': 81, 'disregarded': 82, 'explaining': 83, 'her': 84, 'inspires': 85, 'helping': 86, 'discover': 87, 'potential': 88, 'breakthrough': 89, 'field': 90, 'meanwhile': 91, 'too': 92, 'busy': 93, 'family': 94, 'be': 95, 'brings': 96, 'bert': 97, 'annoys': 98, 'bernadette': 99, 'by': 100, 'writing': 101, 'an': 102, 'astronaut': 103, 'themed': 104, 'musical': 105, 'while': 106, 'bed': 107, 'rest': 108, 'rejoin': 109, 'three': 110, 'are': 111, 'poorly': 112, 'received': 113, 'at': 114, 'bar': 115, 'mitzvah': 116, 'after': 117, 'singing': 118, \"bert's\": 119, 'original': 120, 'song': 121, 'about': 122, 'boulder': 123, 'raiders': 124, 'lost': 125, 'ark': 126} \n\n(7, 127)\ntokenised document data:\n0 document length: 26\n1 document length: 28\n2 document length: 27\n3 document length: 41\n4 document length: 20\n5 document length: 23\n6 document length: 23\n\npadded documents:\n [[  3  20  11  12  13  14  32   6  33  21  34  35  36   7   8  37   9  38\n    4  39  40  41  42  22   9  43   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n [  4  23  14  44   2  45  46  10  47  48   6   4  49  50  15  51  52  24\n    3  53  54  13  55   5  56  25  10  57   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n [ 20  13  14  58  59  60  61  16  26  62  63  64  24  65  27   6  17  66\n    3   7  25   4  67  68   5  69  70   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n [ 27  71  18  72  73   5  74  28  75  76   8   5  77  78  16  29  30  22\n    7  79   4  80  81  82  15  83  29  30   9  84  85   7  86  18  87  10\n   88  89  19   2  90   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n [ 91  11   8  92  93   3   5  94   9  95  19   2  31   3  12   6  12  96\n   19  97   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n [ 15  26  11  98  99 100 101 102 103 104 105 106  17   8  21 107 108  17\n   23  18 109   2  31   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n [  2 110 111 112 113 114  10 115 116 117 118 119 120 121 122   2 123  16\n  124  28   2 125 126   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n\n(7, 68)\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\n\nClassification Model w/ Embedding Layer\n\n'''\n\nvocab_size = 127\nmax_length = 68\n\n# define the model\nprint('\\nBinary Classification Model\\n')\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 8, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy', \n              metrics=['accuracy'])\nprint(model.summary())\n\n# # fit the model\nprint('\\nTraining:\\n')\nmodel.fit(padded_docs, \n          labels, \n          epochs=5, \n          verbose=1)\n\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels,verbose=1)\n\nprint('\\nEvaluation\\n')\nprint('Accuracy: %f' % (accuracy*100))\n","metadata":{"execution":{"iopub.status.busy":"2022-12-18T08:24:36.396676Z","iopub.execute_input":"2022-12-18T08:24:36.397153Z","iopub.status.idle":"2022-12-18T08:24:37.423890Z","shell.execute_reply.started":"2022-12-18T08:24:36.397115Z","shell.execute_reply":"2022-12-18T08:24:37.422393Z"},"trusted":true},"execution_count":163,"outputs":[{"name":"stdout","text":"\nBinary Classification Model\n\nModel: \"sequential_26\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_37 (Embedding)     (None, 68, 8)             1016      \n_________________________________________________________________\nflatten_35 (Flatten)         (None, 544)               0         \n_________________________________________________________________\ndense_44 (Dense)             (None, 1)                 545       \n=================================================================\nTotal params: 1,561\nTrainable params: 1,561\nNon-trainable params: 0\n_________________________________________________________________\nNone\n\nTraining:\n\nEpoch 1/5\n1/1 [==============================] - 1s 664ms/step - loss: 0.6997 - accuracy: 0.4286\nEpoch 2/5\n1/1 [==============================] - 0s 3ms/step - loss: 0.6940 - accuracy: 0.4286\nEpoch 3/5\n1/1 [==============================] - 0s 5ms/step - loss: 0.6884 - accuracy: 0.5714\nEpoch 4/5\n1/1 [==============================] - 0s 5ms/step - loss: 0.6828 - accuracy: 0.7143\nEpoch 5/5\n1/1 [==============================] - 0s 5ms/step - loss: 0.6773 - accuracy: 0.7143\n1/1 [==============================] - 0s 179ms/step - loss: 0.6719 - accuracy: 0.8571\n\nEvaluation\n\nAccuracy: 85.714287\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Generating Dataset\n\nIf we need to utilise tensorflow `dataset`","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\n\nldf = pd.DataFrame(encoded_docs)\nldf.fillna(0,inplace=True)\nldf = ldf.astype('int')\n\ntext_encoded = ldf.values\nlabels = np.array([0,1,1,0,0,0,1])\nprint(text_encoded.shape)\nprint(labels.shape)\n\n# tf.convert_to_tensor(encoded_docs)\ndataset = tf.data.Dataset.from_tensor_slices((text_encoded,labels))\ndataset","metadata":{"execution":{"iopub.status.busy":"2022-12-18T08:25:29.641670Z","iopub.execute_input":"2022-12-18T08:25:29.642086Z","iopub.status.idle":"2022-12-18T08:25:29.659351Z","shell.execute_reply.started":"2022-12-18T08:25:29.642054Z","shell.execute_reply":"2022-12-18T08:25:29.657991Z"},"trusted":true},"execution_count":164,"outputs":[{"name":"stdout","text":"(7, 41)\n(7,)\n","output_type":"stream"},{"execution_count":164,"output_type":"execute_result","data":{"text/plain":"<TensorSliceDataset shapes: ((41,), ()), types: (tf.int64, tf.int64)>"},"metadata":{}}]}]}
