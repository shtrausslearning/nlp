{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Load Glove Data\n\n`embeddings_index` - dictionary stores word & vector data, loaded from file","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nembeddings_index = {}\n\nf = open('/kaggle/input/glovedata/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n\nf.close()\nprint(f'Loaded {len(embeddings_index)} word vectors.')","metadata":{"execution":{"iopub.status.busy":"2022-12-20T04:18:22.970396Z","iopub.execute_input":"2022-12-20T04:18:22.971671Z","iopub.status.idle":"2022-12-20T04:18:33.447132Z","shell.execute_reply.started":"2022-12-20T04:18:22.971534Z","shell.execute_reply":"2022-12-20T04:18:33.445074Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Loaded 400000 word vectors.\n","output_type":"stream"}]},{"cell_type":"code","source":"# sample glove embedding\nembeddings_index['the']","metadata":{"execution":{"iopub.status.busy":"2022-12-20T04:22:50.765069Z","iopub.execute_input":"2022-12-20T04:22:50.765452Z","iopub.status.idle":"2022-12-20T04:22:50.773692Z","shell.execute_reply.started":"2022-12-20T04:22:50.765422Z","shell.execute_reply":"2022-12-20T04:22:50.772685Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Load Corpus Data","metadata":{}},{"cell_type":"code","source":"# Load Corpus\ndocs = ['Well done!',\n        'Good work',\n        'Great effort',\n        'nice work',\n        'Excellent!',\n        'Weak',\n        'Poor effort!',\n        'not good',\n        'poor work',\n        'Could have done better.']\n\n# Define class labels\nlabels = np.array([1,1,1,1,1,0,0,0,0,0])","metadata":{"execution":{"iopub.status.busy":"2022-12-20T04:18:33.448916Z","iopub.execute_input":"2022-12-20T04:18:33.449480Z","iopub.status.idle":"2022-12-20T04:18:33.457270Z","shell.execute_reply.started":"2022-12-20T04:18:33.449415Z","shell.execute_reply":"2022-12-20T04:18:33.455624Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Tokenise Documents\n\n`encoded_docs` - tokenised data (non padded)","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\n# Prepare tokenizer\ntokeniser = Tokenizer()\ntokeniser.fit_on_texts(docs)\nvocab_size = len(tokeniser.word_index) + 1\n\n# Integer encode the documents\nencoded_docs = tokeniser.texts_to_sequences(docs)\nprint(f'encoded documents: {encoded_docs}')","metadata":{"execution":{"iopub.status.busy":"2022-12-20T04:20:46.895593Z","iopub.execute_input":"2022-12-20T04:20:46.896204Z","iopub.status.idle":"2022-12-20T04:20:54.637435Z","shell.execute_reply.started":"2022-12-20T04:20:46.896162Z","shell.execute_reply":"2022-12-20T04:20:54.636419Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"encoded documents: [[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Check Tokenisation Vocabulary\n\n`c` - contains dictionary word,id pairs for encoded tokens ","metadata":{}},{"cell_type":"code","source":"# Check Vocabulary Dictionary\nc = tokeniser.word_index\nprint(c)\nprint('\\n',len(c),'words in dictionary')","metadata":{"execution":{"iopub.status.busy":"2022-12-20T04:21:05.744112Z","iopub.execute_input":"2022-12-20T04:21:05.745454Z","iopub.status.idle":"2022-12-20T04:21:05.753162Z","shell.execute_reply.started":"2022-12-20T04:21:05.745386Z","shell.execute_reply":"2022-12-20T04:21:05.751679Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'work': 1, 'done': 2, 'good': 3, 'effort': 4, 'poor': 5, 'well': 6, 'great': 7, 'nice': 8, 'excellent': 9, 'weak': 10, 'not': 11, 'could': 12, 'have': 13, 'better': 14}\n\n 14 words in dictionary\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Extract Glove Vectors for vocabulary contents\n\n`glove.6B.100d.txt` - word embedding w/ **100 dimensions**\n\n`embedding_matrix` - found glove embedding vector for all words in our vocabulary dictionary `c`","metadata":{}},{"cell_type":"code","source":"# Create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size, 100))\n\n# Cycle through all words in tokenised dictionary (could miss non existent)\nfor word, i in c.items():\n    embedding_vector = embeddings_index[word]  # get current word embedding\n    if embedding_vector is not None:           \n        embedding_matrix[i] = embedding_vector # if found add it\n\nprint(f'embedding dimension: {embedding_matrix.shape}')\nembedding_matrix","metadata":{"execution":{"iopub.status.busy":"2022-12-20T04:26:29.381351Z","iopub.execute_input":"2022-12-20T04:26:29.381859Z","iopub.status.idle":"2022-12-20T04:26:29.394507Z","shell.execute_reply.started":"2022-12-20T04:26:29.381822Z","shell.execute_reply":"2022-12-20T04:26:29.393247Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"embedding dimension: (15, 100)\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.11619   ,  0.45447001, -0.69216001, ..., -0.54737002,\n         0.48822001,  0.32246   ],\n       [-0.2978    ,  0.31147   , -0.14937   , ..., -0.22709   ,\n        -0.029261  ,  0.4585    ],\n       ...,\n       [ 0.05869   ,  0.40272999,  0.38633999, ..., -0.35973999,\n         0.43718001,  0.10121   ],\n       [ 0.15711001,  0.65605998,  0.0021149 , ..., -0.60614997,\n         0.71004999,  0.41468999],\n       [-0.047543  ,  0.51914001,  0.34283999, ..., -0.26859   ,\n         0.48664999,  0.55609   ]])"},"metadata":{}}]},{"cell_type":"markdown","source":"### Pad Encoded Document Data\n\nEncoded corpus `encoded_docs` contains encoded documents of different length, set common length by padding","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokeniser.word_index\n\nmax_length = 4\n# Pad documents to a max length of 4 words\npadded_docs = pad_sequences(encoded_docs, \n                            maxlen=max_length, \n                            padding='post')\nprint(f'padded documents: \\n\\n{padded_docs}')","metadata":{"execution":{"iopub.status.busy":"2022-12-20T04:34:40.452478Z","iopub.execute_input":"2022-12-20T04:34:40.453651Z","iopub.status.idle":"2022-12-20T04:34:40.462816Z","shell.execute_reply.started":"2022-12-20T04:34:40.453601Z","shell.execute_reply":"2022-12-20T04:34:40.461193Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"padded documents: \n\n[[ 6  2  0  0]\n [ 3  1  0  0]\n [ 7  4  0  0]\n [ 8  1  0  0]\n [ 9  0  0  0]\n [10  0  0  0]\n [ 5  4  0  0]\n [11  3  0  0]\n [ 5  1  0  0]\n [12 13  2 14]]\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Flatten, Dense\n\n# Define the model\nmodel = Sequential()\n\n# Use GloVe weights (frozen, non trainable)\nemb_layer = Embedding(input_dim=vocab_size,         # Input into embedding layer, c (vocab size)\n                      output_dim = 100,             # Output out of embedding layer (100 dimensions)\n                      weights=[embedding_matrix],   # Custom weights (define custom weights)\n                      input_length=max_length,      # Input length (padding size)       \n                      trainable=False)              # Trainable weights in layer \n\n# # Trainable Embedding Layer\n# emb_layer = Embedding(input_dim=vocab_size,  \n#                       output_dim = 8, \n#                       input_length=max_length,\n#                       trainable=True)\n\nmodel.add(emb_layer) # embedding layer\nmodel.add(Flatten()) # flatten embedding layer\nmodel.add(Dense(1, activation='sigmoid')) # binary classification \n\n# compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# summarize the model\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2022-12-20T04:37:05.981142Z","iopub.execute_input":"2022-12-20T04:37:05.981900Z","iopub.status.idle":"2022-12-20T04:37:06.041925Z","shell.execute_reply.started":"2022-12-20T04:37:05.981852Z","shell.execute_reply":"2022-12-20T04:37:06.040097Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 4, 100)            1500      \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 401       \n=================================================================\nTotal params: 1,901\nTrainable params: 401\nNon-trainable params: 1,500\n_________________________________________________________________\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"# fit the model\nmodel.fit(padded_docs, labels,\n          epochs=50,\n          verbose=0)\n\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, \n                                labels,\n                                verbose=0)\n\n# training accuracy                     \nprint(f'Accuracy: {accuracy*100:.5f}')","metadata":{"execution":{"iopub.status.busy":"2022-12-20T04:37:09.584700Z","iopub.execute_input":"2022-12-20T04:37:09.585420Z","iopub.status.idle":"2022-12-20T04:37:10.878796Z","shell.execute_reply.started":"2022-12-20T04:37:09.585373Z","shell.execute_reply":"2022-12-20T04:37:10.877283Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"2022-12-20 04:37:09.682861: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 100.00000\n","output_type":"stream"}]}]}