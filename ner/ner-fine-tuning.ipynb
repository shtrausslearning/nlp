{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport torch\nfrom tqdm import tqdm\nfrom importlib import reload\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-10T11:11:02.991498Z","iopub.execute_input":"2023-08-10T11:11:02.991984Z","iopub.status.idle":"2023-08-10T11:11:06.938527Z","shell.execute_reply.started":"2023-08-10T11:11:02.991938Z","shell.execute_reply":"2023-08-10T11:11:06.937126Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"No GPU available, using the CPU instead.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\n\n# data = load_dataset('AmazonScience/massive')\n#data = Dataset.from_pandas(dict(tuple(data['train'].to_pandas().groupby('locale')))['ru-RU'])\n#data = load_dataset('AmazonScience/massive', split='train[345420:345520]')\n# dataset = dataset.train_test_split(test_size=0.1)\n# data = data.remove_columns('__index_level_0__')\n# data = data.select(range(100))\n\ntrain_dataset = load_dataset('AmazonScience/massive', \"ru-RU\",split=\"train[:10]\")\ntest_dataset = load_dataset('AmazonScience/massive', \"ru-RU\",split=\"test[:10]\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:11:06.940831Z","iopub.execute_input":"2023-08-10T11:11:06.941819Z","iopub.status.idle":"2023-08-10T11:11:18.992655Z","shell.execute_reply.started":"2023-08-10T11:11:06.941779Z","shell.execute_reply":"2023-08-10T11:11:18.991410Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/30.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdcb5958ba154aa4966471e2dee9a99a"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset massive/ru-RU to /root/.cache/huggingface/datasets/AmazonScience___massive/ru-RU/1.0.0/71d360eb7d7a18565ff8c10609cebf714fce3cc390e173ba5b02ffd48543cdc1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/40.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baaae83cc37b4387a711ed2792c5d169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset massive downloaded and prepared to /root/.cache/huggingface/datasets/AmazonScience___massive/ru-RU/1.0.0/71d360eb7d7a18565ff8c10609cebf714fce3cc390e173ba5b02ffd48543cdc1. Subsequent calls will reuse this data.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(train_dataset),len(test_dataset))","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:11:18.994343Z","iopub.execute_input":"2023-08-10T11:11:18.994886Z","iopub.status.idle":"2023-08-10T11:11:19.000751Z","shell.execute_reply.started":"2023-08-10T11:11:18.994851Z","shell.execute_reply":"2023-08-10T11:11:18.999636Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"10 10\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset['utt'][:10] ","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:11:19.003240Z","iopub.execute_input":"2023-08-10T11:11:19.004279Z","iopub.status.idle":"2023-08-10T11:11:19.021799Z","shell.execute_reply.started":"2023-08-10T11:11:19.004238Z","shell.execute_reply":"2023-08-10T11:11:19.019871Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['разбуди меня в девять утра в пятницу',\n 'поставь будильник на два часа вперед',\n 'олли тихо',\n 'отстановись',\n 'олли остановись на десять секунд',\n 'остановись на десять секунд',\n 'сделай освещение здесь чуть более тёплым',\n 'пожалуйста сделай свет подходящий для чтения',\n 'время идти спать',\n 'олли время спать']"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset['annot_utt'][:10]","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:11:19.023490Z","iopub.execute_input":"2023-08-10T11:11:19.024186Z","iopub.status.idle":"2023-08-10T11:11:19.037875Z","shell.execute_reply.started":"2023-08-10T11:11:19.024151Z","shell.execute_reply":"2023-08-10T11:11:19.036789Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['разбуди меня в [time : девять утра] в [date : пятницу]',\n 'поставь будильник [time : на два часа вперед]',\n 'олли тихо',\n 'отстановись',\n 'олли остановись на [time : десять секунд]',\n 'остановись на [time : десять секунд]',\n 'сделай освещение здесь чуть более [color_type : тёплым]',\n 'пожалуйста сделай свет [color_type : подходящий для чтения]',\n 'время идти спать',\n 'олли время спать']"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\nMODEL = AutoModel.from_pretrained(\"ai-forever/ruBert-base\")\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")                         \n# model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n# model = AutoModel.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:11:19.040023Z","iopub.execute_input":"2023-08-10T11:11:19.041444Z","iopub.status.idle":"2023-08-10T11:11:37.169512Z","shell.execute_reply.started":"2023-08-10T11:11:19.041395Z","shell.execute_reply":"2023-08-10T11:11:37.167977Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/590 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"489ce7da728f47b59fa2b15d70e8c11a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/1.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b20b6631aa349bc99d7f58596e2125c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/716M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dba16030b1f445a92460e9ffa986632"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at ai-forever/ruBert-base were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"from typing import List\nimport regex as re\n\n'''\n\nPARSER FOR THE DATASET NER TAG FORMAT\n\n'''\n\nclass Parser:\n    \n    # RE patterns for tag extraction\n    LABEL_PATTERN = r\"\\[(.*?)\\]\"\n    PUNCTUATION_PATTERN = r\"([.,\\/#!$%\\^&\\*;:{}=\\-_`~()'\\\"’¿])\"\n    \n    # initialise, first word/id tag is O (outside)\n    def __init__(self):\n        self.tag_to_id = {\n            \"O\": 0\n        }\n        self.id_to_tag = {\n            0: \"O\"\n        }\n        \n    '''\n    \n    CREATE TAGS\n    \n    '''\n    # input : sentence, tagged sentence\n    \n    def __call__(self, sentence: str, annotated: str) -> List[str]:\n    \n        ''' Create Dictionary of Identified Tags'''\n    \n        # 1. set label B or I    \n        \n        matches = re.findall(self.LABEL_PATTERN, annotated)\n        word_to_tag = {}\n        for match in matches:\n            tag, phrase = match.split(\" : \")\n            words = phrase.split(\" \") \n            word_to_tag[words[0]] = f\"B-{tag.upper()}\"\n            for w in words[1:]:\n                word_to_tag[w] = f\"I-{tag.upper()}\"\n                \n        ''' Tokenise Sentence & add tags to not tagged words (O)'''\n        \n        # 2. add token tag to main tag dictionary\n\n        tags = []\n        sentence = re.sub(self.PUNCTUATION_PATTERN, r\" \\1 \", sentence)\n        for w in sentence.split():\n            if w not in word_to_tag:\n                tags.append(\"O\")\n            else:\n                tags.append(word_to_tag[w])\n                self.__add_tag(word_to_tag[w])\n        \n        return tags\n    \n    '''\n    \n    TAG CONVERSION\n    \n    '''\n    # to word2id (tag_to_id)\n    # to id2word (id_to_tag)\n\n    def __add_tag(self, tag: str):\n        if tag in self.tag_to_id:\n            return\n        id_ = len(self.tag_to_id)\n        self.tag_to_id[tag] = id_\n        self.id_to_tag[id_] = tag\n        \n    ''' Get Tag Number ID '''\n    # or just number id for token\n        \n    def get_id(self, tag: str):\n        return self.tag_to_id[tag]\n    \n    ''' Get Tag Token from Number ID'''\n    # given id get its token\n    \n    def get_label(self, id_: int):\n        return self.get_tag_label(id_)\n    \nparser = Parser()\nparser(train_dataset[\"utt\"][0], train_dataset[\"annot_utt\"][0])","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:11:37.171096Z","iopub.execute_input":"2023-08-10T11:11:37.171448Z","iopub.status.idle":"2023-08-10T11:11:37.190316Z","shell.execute_reply.started":"2023-08-10T11:11:37.171416Z","shell.execute_reply":"2023-08-10T11:11:37.188595Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['O', 'O', 'O', 'B-TIME', 'I-TIME', 'O', 'B-DATE']"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai-forever/ruBert-base\")\nMODEL = AutoModel.from_pretrained(\"ai-forever/ruBert-base\")\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")                         \n# model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n# model = AutoModel.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:11:37.192062Z","iopub.execute_input":"2023-08-10T11:11:37.192461Z","iopub.status.idle":"2023-08-10T11:11:39.995343Z","shell.execute_reply.started":"2023-08-10T11:11:37.192424Z","shell.execute_reply":"2023-08-10T11:11:39.993680Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at ai-forever/ruBert-base were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass NERDataset(Dataset):\n    def __init__(self, dataset, tokenizer):\n        self.tokenizer = tokenizer\n        self.processed_data = self.__preprocess(dataset)\n    \n    def __len__(self):\n        return len(self.processed_data)\n    \n    def __getitem__(self, idx):\n        return self.processed_data[idx]\n    \n    def __preprocess(self, dataset):\n        \n        tmp = {}\n        for idx in tqdm(range(len(dataset))):\n            item = dataset[idx]\n            tags = parser(item[\"utt\"], item[\"annot_utt\"])     # get list of tags\n            tokenizer_output = self.tokenizer(item[\"utt\"],    # tokenise document (incl. <bos>,<eos>)\n                                              padding=True, \n                                              truncation=True, \n                                              return_tensors='pt')\n            \n            # token word identifier (each word can have multiple tokens)\n            word_ids = tokenizer_output.word_ids() \n            \n            # for each word, how many subtokens are there (starts with 1 - first word)\n            subword_group = [\n                (key + 1, len(list(group))) \n                for key, group in itertools.groupby(word_ids) \n                    if key is not None\n            ] # index to aggregate tokens\n\n            # define bio tags for each word in numerical format using parser\n            target = [parser.get_id(t) for t in tags] \n            \n            # group all relevant data that will be used in forward pass\n            tmp[idx] = {\n                **tokenizer_output,\n                \"subword_group\": torch.tensor(subword_group),\n                \"target\": torch.tensor(target)\n            }\n            \n            # check consistency\n            try:\n                assert (len(subword_group) == len(target))\n            except:\n                print(item[\"annot_utt\"], subword_group, target)\n                \n        return tmp\n\ntrain = NERDataset(train_dataset, tokenizer)\ntest = NERDataset(test_dataset, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:11:39.997222Z","iopub.execute_input":"2023-08-10T11:11:39.997642Z","iopub.status.idle":"2023-08-10T11:11:40.055038Z","shell.execute_reply.started":"2023-08-10T11:11:39.997607Z","shell.execute_reply":"2023-08-10T11:11:40.053646Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n100%|██████████| 10/10 [00:00<00:00, 445.84it/s]\n100%|██████████| 10/10 [00:00<00:00, 1332.20it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import PreTrainedModel\nfrom transformers import PretrainedConfig\nfrom transformers import AutoModel, AutoConfig\n\nclass MyConfig(PretrainedConfig):\n    model_type = 'mymodel'\n    def __init__(self, important_param=42, **kwargs):\n        super().__init__(**kwargs)\n        self.important_param = important_param\n\n# PreTrainedModel has nn.Module\n\nclass NERClassifier(PreTrainedModel):\n    \n    config_class = MyConfig\n    def __init__(self,config):\n        super().__init__(config)\n        self.bert = MODEL\n        self.seq = nn.Sequential(\n            nn.Linear(768, 256), \n            nn.ReLU(),\n            nn.Linear(256, CLASSES),\n        )\n        \n    def forward(self, inputs): # returns list of targets\n        \n        # standard inputs for BERT\n        bert_output = self.bert(\n            inputs[\"input_ids\"],\n            inputs[\"attention_mask\"]\n        )\n        \n        # output of transformer encoder will be our hidden state for \n        # each input_ids \n        last_hidden_state = bert_output[\"last_hidden_state\"]\n        \n        # tokens correspond tokenizer divisions \n        # each word can be split into multiple tokens\n        # ie. get the mean word embedding \n        \n        target = []\n        for group in inputs[\"subword_group\"]:\n            b, e = group\n            word_embedding = last_hidden_state[:, b:b+e]       # get the token embeddings\n            agg_embedding = torch.mean(word_embedding, dim=1)  # mean word embeddings for tokens\n            \n            # input mean word embedding (1,768) pass into nn.Sequential linear tail end\n            proba = self.__forward_one(agg_embedding)    # logits data \n            target.append(proba)\n        \n        word_logits = torch.stack(target).squeeze(1)\n        \n        return word_logits\n        \n    def __forward_one(self, x):\n        logits = self.seq(x)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:11:40.058375Z","iopub.execute_input":"2023-08-10T11:11:40.058753Z","iopub.status.idle":"2023-08-10T11:11:40.069077Z","shell.execute_reply.started":"2023-08-10T11:11:40.058720Z","shell.execute_reply":"2023-08-10T11:11:40.067838Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\n# define data loaders\ntrain_loader = DataLoader(train)\ntest_loader = DataLoader(test)\nCLASSES = len(parser.tag_to_id); print(f'{CLASSES} labels')\nconfig = MyConfig(4)\n\n# define classifier model, loss fucntion & optimiser\nclf = NERClassifier(config).to(device)\ncriterion = nn.CrossEntropyLoss()  # for multiclass classification \noptimizer = optim.Adam(clf.parameters(), lr=1e-5) ","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:11:40.071150Z","iopub.execute_input":"2023-08-10T11:11:40.071732Z","iopub.status.idle":"2023-08-10T11:11:40.104528Z","shell.execute_reply.started":"2023-08-10T11:11:40.071680Z","shell.execute_reply":"2023-08-10T11:11:40.103650Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"9 labels\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nfor epoch in range(10):\n    \n    '''\n    \n    (1) TRAINING LOOP\n    \n    '''\n    \n    loss_count, loss_sum = 0, 0\n    y_true, y_pred = [], []\n    \n    # switch to training mode, ie backpropagation on\n    clf.train()\n    for data in tqdm(train_loader):\n        \n        # move data to device\n        inputs = {\n            key: val.squeeze(0).to(device)\n            for key, val in data.items()\n        }\n        \n        # logits of belonging to each of the tag class \n        # for all words in document\n        outputs = clf(inputs)\n        \n        # predicted word tag \n        word_tag = torch.argmax(outputs, dim=1).tolist() \n        \n        y_true.extend(inputs[\"target\"].tolist())\n        y_pred.extend(word_tag)        \n        \n        # calcualate loss\n        loss = criterion(outputs, inputs[\"target\"])\n        loss_count += 1\n        loss_sum += loss.item()\n        \n#         nn.utils.clip_grad_norm_(\n#             parameters=clf.parameters(), max_norm=20\n#         )\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    print(f\"Epoch-{epoch + 1}: loss: {loss_sum / loss_count}; acc: {accuracy_score(y_true, y_pred)}\")\n        \n    '''\n    \n    (2) VALIDATION LOOP\n    \n    '''\n        \n    test_loss_sum, test_loss_count = 0, 0\n    test_true, test_pred = [], []\n    \n    # switch to inference mode\n    with torch.no_grad():\n        for test_rows in tqdm(test_loader):\n            \n            # move data to device\n            test_inputs = {\n                key: val.squeeze(0).to(device)\n                for key, val in test_rows.items()\n            }\n            test_outputs = clf(test_inputs)\n\n            # add metric data\n            test_true.extend(test_inputs[\"target\"].tolist())\n            test_pred.extend(torch.argmax(test_outputs, dim=1).tolist())\n\n            test_loss = criterion(test_outputs, test_inputs[\"target\"])\n            test_loss_count += 1\n            test_loss_sum += test_loss.item()\n        \n        \n    print(f\"Epoch-{epoch + 1}: loss: {loss_sum / loss_count}; acc: {accuracy_score(y_true, y_pred)},\\\n          val_loss: {test_loss_sum / test_loss_count}, val_acc: {accuracy_score(test_true, test_pred)}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:38:43.652823Z","iopub.execute_input":"2023-08-10T11:38:43.653206Z","iopub.status.idle":"2023-08-10T11:40:36.702558Z","shell.execute_reply.started":"2023-08-10T11:38:43.653181Z","shell.execute_reply":"2023-08-10T11:40:36.700928Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"100%|██████████| 10/10 [00:10<00:00,  1.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-1: loss: 0.6805069640278816; acc: 0.6976744186046512\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 10.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-1: loss: 0.6805069640278816; acc: 0.6976744186046512,          val_loss: 0.8717933624982834, val_acc: 0.813953488372093\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:10<00:00,  1.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-2: loss: 0.635441517829895; acc: 0.7906976744186046\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:01<00:00,  9.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-2: loss: 0.635441517829895; acc: 0.7906976744186046,          val_loss: 0.8502975344657898, val_acc: 0.813953488372093\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:10<00:00,  1.06s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-3: loss: 0.591285602748394; acc: 0.7674418604651163\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 10.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-3: loss: 0.591285602748394; acc: 0.7674418604651163,          val_loss: 0.8234651923179627, val_acc: 0.8372093023255814\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:10<00:00,  1.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-4: loss: 0.5393777281045914; acc: 0.8372093023255814\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 11.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-4: loss: 0.5393777281045914; acc: 0.8372093023255814,          val_loss: 0.7809332594275474, val_acc: 0.8372093023255814\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-5: loss: 0.5065406486392021; acc: 0.8372093023255814\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 11.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-5: loss: 0.5065406486392021; acc: 0.8372093023255814,          val_loss: 0.7536620497703552, val_acc: 0.8372093023255814\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-6: loss: 0.4675091072916985; acc: 0.9069767441860465\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 10.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-6: loss: 0.4675091072916985; acc: 0.9069767441860465,          val_loss: 0.7254292577505111, val_acc: 0.8604651162790697\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-7: loss: 0.4321024067699909; acc: 0.9302325581395349\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 11.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-7: loss: 0.4321024067699909; acc: 0.9302325581395349,          val_loss: 0.7173458933830261, val_acc: 0.8604651162790697\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-8: loss: 0.4029550924897194; acc: 0.9302325581395349\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 13.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-8: loss: 0.4029550924897194; acc: 0.9302325581395349,          val_loss: 0.692103661596775, val_acc: 0.8604651162790697\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-9: loss: 0.3754826165735722; acc: 0.9534883720930233\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 10.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-9: loss: 0.3754826165735722; acc: 0.9534883720930233,          val_loss: 0.6928185507655144, val_acc: 0.8604651162790697\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:10<00:00,  1.04s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch-10: loss: 0.3559957958757877; acc: 0.9534883720930233\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 11.35it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch-10: loss: 0.3559957958757877; acc: 0.9534883720930233,          val_loss: 0.6674651488661766, val_acc: 0.8604651162790697\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"clf.save_pretrained('./my_model_dir')\ntokenizer.save_pretrained('./my_model_dir')","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:41:38.449852Z","iopub.execute_input":"2023-08-10T11:41:38.452453Z","iopub.status.idle":"2023-08-10T11:41:40.088035Z","shell.execute_reply.started":"2023-08-10T11:41:38.452388Z","shell.execute_reply":"2023-08-10T11:41:40.086567Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"('./my_model_dir/tokenizer_config.json',\n './my_model_dir/special_tokens_map.json',\n './my_model_dir/vocab.txt',\n './my_model_dir/added_tokens.json',\n './my_model_dir/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"new_model = NERClassifier.from_pretrained('./my_model_dir')\nnew_model","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:42:20.564926Z","iopub.execute_input":"2023-08-10T11:42:20.565297Z","iopub.status.idle":"2023-08-10T11:42:21.322140Z","shell.execute_reply.started":"2023-08-10T11:42:20.565266Z","shell.execute_reply":"2023-08-10T11:42:21.320195Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"NERClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(120138, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (seq): Sequential(\n    (0): Linear(in_features=768, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=9, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def ner_inference(text,model):\n\n    # Tokenise input\n    tokenizer_output = tokenizer(text,    \n                                 padding=True, \n                                 truncation=True, \n                                 return_tensors='pt')\n\n    # token word identifier (each word can have multiple tokens)\n    word_ids = tokenizer_output.word_ids() \n\n    # for each word, how many subtokens are there (starts with 1 - first word)\n    subword_group = [\n        (key + 1, len(list(group))) \n        for key, group in itertools.groupby(word_ids) \n            if key is not None\n    ] # index to aggregate tokens\n\n    # group all relevant data that will be used in forward pass \n    output = {\n        **tokenizer_output,\n        \"subword_group\": torch.tensor(subword_group),\n    }\n    \n    # get the highest logits value\n    tag_pred = torch.argmax(model(output),axis=1).tolist()\n    tag_pred \n    \n    return tag_pred\n\nner_inference('В девять утра я улетаю в Зимбабве',new_model)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T11:42:27.965313Z","iopub.execute_input":"2023-08-10T11:42:27.965798Z","iopub.status.idle":"2023-08-10T11:42:28.120172Z","shell.execute_reply.started":"2023-08-10T11:42:27.965757Z","shell.execute_reply":"2023-08-10T11:42:28.119061Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"[0, 2, 0, 0, 0, 0, 0]"},"metadata":{}}]}]}