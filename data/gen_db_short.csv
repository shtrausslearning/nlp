id,data

#
#    Python | 
#

db_pyt0,"
1. strip() - removes leading and trailing whitespace from a string
2. replace() - replaces a substring with another substring
3. lower() - converts all characters in a string to lowercase
4. upper() - converts all characters in a string to uppercase
5. split() - splits a string into a list of substrings based on a delimiter
6. join() - joins a list of strings into a single string using a specified delimiter
7. isalpha() - checks if a string contains only alphabetic characters
8. isnumeric() - checks if a string contains only numeric characters
9. isalnum() - checks if a string contains only alphanumeric characters
10. isspace() - checks if a string contains only whitespace characters"

#
#
#    GridSearchCV
#
#

db_gs0,"
<code>
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# Define the hyperparameters to tune
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10]
}

# Create a random forest classifier
rfc = RandomForestClassifier()

# Use GridSearchCV to search the hyperparameter space
grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and score
print('Best hyperparameters: ', grid_search.best_params_)
print('Best score: ', grid_search.best_score_)
</code>"

#
#
#    CatBoost Database
#
#

db_cb0,"
<code>
from catboost import CatBoostClassifier

# Load the dataset
X_train, y_train = load_data('train.csv')
X_test, y_test = load_data('test.csv')

# Initialize the classifier
clf = CatBoostClassifier()

# Train the classifier
clf.fit(X_train, y_train)

# Evaluate the classifier on the test data
accuracy = clf.score(X_test, y_test)
print('Accuracy:', accuracy)
</code>"

db_cb1,"
<code>
from catboost import CatBoostRegressor

# Load the dataset
X_train, y_train = load_data('train.csv')
X_test, y_test = load_data('test.csv')

# Initialize the regressor
regressor = CatBoostRegressor()

# Train the regressor
regressor.fit(X_train, y_train)

# Evaluate the regressor on the test data
rmse = np.sqrt(mean_squared_error(y_test, regressor.predict(X_test)))
print('RMSE:', rmse)
</code>"

db_cb2,"
<code>
from catboost import CatBoostClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the classifier
classifier = CatBoostClassifier(loss_function='MultiClass')

# Train the classifier
classifier.fit(X_train, y_train)

# Evaluate the classifier on the test data
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
</code>"

db_cb3,"none"
db_cb4,"
- <b>learning_rate</b>: The learning rate determines the step size at each iteration while moving toward a minimum of the loss function. Lower  values can improve accuracy, but require more iterations. 
- <b>depth</b>: The depth of the decision tree used in the model. Increasing the depth can improve accuracy, but also increases the risk of overfitting. 
- <b>l2_leaf_reg</b>: The L2 regularization coefficient. Increasing this value can help prevent overfitting. 
- <b>iterations</b>: The number of iterations to run during training. Increasing this value can improve accuracy, but also increases training time.
- <b>random_strength</b>: The amount of randomness to use when selecting splits in the decision tree. Higher values can increase the model's 
- <b>border_count</b>: The number of splits to consider when building a decision tree. Increasing this value can improve accuracy, but also increases training time'
- <b>loss_function</b>: The loss function to optimize during training. For regression tasks, common options include MAE (mean absolute error), RMSE (root mean squared error), and Quantile (for quantile regression).
- <b>task_type</b>: The type of task to perform, either CPU or GPU. Using a GPU can significantly speed up training time. 
- <b>boosting_type</b>: The type of boosting to use, either Plain or Ordered. The Ordered option can be useful for datasets with categorical features. 
- <b>colsample_bylevel</b>: The fraction of features to use at each level of the decision tree.
- <b>min_data_in_leaf</b>: The minimum number of samples required in each leaf node of the decision tree. Increasing this value can help prevent overfitting."

db_cb5,"
Here are some steps that can help you get started:

[1] <b>Install CatBoost</b>: Install CatBoost if you already haven't in your terminal/cell

pip install catboost

[2] <b>Import CatBoost</b>:

from catboost import CatBoostClassifier

[3] <b>Prepare your data</b>: Before training your model, you need to prepare your data, such as cleaning, preprocessing, splitting your data into subsets and encoding categorical variables if you have any

[4] <b>Train your model</b>: Once you have prepared your data, you can train your model using the <code>CatBoostClassifier()</code> function. Specify hyperparameters such as learning rate, depth, and number of iterations to tune your model. 

Here is an example of how you can train a CatBoost classifier:

<code>
model = CatBoostClassifier(iterations=1000,
                           learning_rate=0.1,
                           depth=6,
                           loss_function='Logloss',
                           verbose=True)
model.fit(X_train, y_train, cat_features=categorical_features_indices)
</code>

[5] <b>Evaluate your model</b>: After training your model, you can evaluate its performance on the testing set using metrics such as accuracy, precision, recall, and F1 score.

[6] <b>Use your model for predictions</b>: Once you are satisfied with the performance of your model, you can use it for making predictions on new data using the predict() function.

y_pred = model.predict(X_test)"

db_cb6,"
<code>
import pandas as pd
from catboost import CatBoostClassifier

# Load dataset
data = pd.read_csv('dataset.csv')

# Split into features and target variable
X = data.drop('target', axis=1)
y = data['target']

# Define categorical features
cat_features = ['feature1', 'feature2', 'feature3']

# Create CatBoost classifier with categorical features
clf = CatBoostClassifier(cat_features=cat_features)

# Fit the model
clf.fit(X, y)

# Make predictions on new data
new_data = pd.read_csv('new_data.csv')
predictions = clf.predict(new_data)
</code>"

#
#
#    Optuna
#
#

db_opt0,"
<code>
import catboost as cb
from sklearn.metrics import mean_squared_error
import optuna

def objective(trial):
    params = {
        'iterations': 1000,
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),
        'depth': trial.suggest_int('depth', 1, 10),
        'subsample': trial.suggest_float('subsample', 0.05, 1.0),
        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.05, 1.0),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),
    }

    model = cb.CatBoostRegressor(**params, silent=True)
    model.fit(X_train, y_train)
    predictions = model.predict(X_val)
    rmse = mean_squared_error(y_val, predictions, squared=False)
    return rmse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=30)
print('Best hyperparameters:', study.best_params)
print('Best RMSE:', study.best_value)
</code>"

db_opt1,"
[1] Define your objective function : This is the function that takes in the hyperparameters as input and returns a scalar value that represents the performance of the model.

[2] Define the hyperparameter search space : This is the range of values that Optuna will explore for each hyperparameter. You can define this using various distributions such as uniform, log-uniform, categorical, etc.

[3] Create a study object : This is the main object that controls the optimization process. You can specify the optimization algorithm and other parameters here.

[4] Run the optimization loop : In this loop, Optuna will suggest new hyperparameters to evaluate based on the previous results. You need to evaluate the performance of the model with each set of hyperparameters and report the result to Optuna.

[5] Retrieve the best hyperparameters : Once the optimisation is complete, you can retrieve the best set of hyperparameters using the study.best_params attribute."

db_opt2,"
<code>
import optuna
from sklearn.datasets import load_iris
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

# create objective function
def objective(trial):
    
    # define the hyperparameter search space
    C = trial.suggest_loguniform('C', 1e-5, 1e5)
    gamma = trial.suggest_loguniform('gamma', 1e-5, 1e5)

    # create a SVM model with the hyperparameters
    clf = SVC(C=C, gamma=gamma)

    # evaluate the model using cross-validation & return mean value
    iris = load_iris()
    score = cross_val_score(clf, iris.data, iris.target, cv=5).mean()

    # Report the result to Optuna
    return score

# create a study object and run the optimization loop
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

# retrieve the best hyperparameters
best_params = study.best_params
print('Best hyperparameters:', best_params)
</code>"

db_opt3,"
<code>
import optuna
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# load the dataset
data = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)

def objective(trial):

    # define the hyperparameters to search over
    params = {
        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),
        'C': trial.suggest_loguniform('C', 1e-4, 1e4),
        'solver': trial.suggest_categorical('solver', ['liblinear', 'saga'])
    }
    
    # create a LogisticRegression model with the given hyperparameters
    model = LogisticRegression(**params)
    
    # train and evaluate the model using cross-validation
    score = cross_val_score(model, X_train, y_train, cv=5).mean()
    
    return score

# Define the study object and run the optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

# Print the best hyperparameters and score found during optimization
print('Best score:', study.best_value)
print('Best params:', study.best_params)
<code>"

db_opt4,"
<code>
import optuna
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# Load the breast cancer dataset
data = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)

def objective(trial):

    # define the hyperparameters to search over
    params = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)
    }
    
    # create a GradientBoostingClassifier model with the given hyperparameters
    model = GradientBoostingClassifier(**params)
    
    # train and evaluate the model using cross-validation
    score = cross_val_score(model, X_train, y_train, cv=5).mean()
    
    return score

# Define the study object and run the optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

# Print the best hyperparameters and score found during optimization
print('Best score:', study.best_value)
print('Best params:', study.best_params)
</code>"

db_opt5,"
<code>
import optuna
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# load the dataset
data = load_boston()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)

def objective(trial):

    # define the hyperparameters to search over
    params = {
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)
    }
    
    # create a GradientBoostingRegressor model with the given hyperparameters
    model = GradientBoostingRegressor(**params)
    
    # train and evaluate the model using cross-validation
    score = cross_val_score(model, X_train, y_train, cv=5).mean()
    
    return score

# define the study object and run the optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

# print the best hyperparameters and score found during optimization
print('Best score:', study.best_value)
print('Best params:', study.best_params)
</code>"

db_opt6,"
<code>
import optuna
from sklearn.ensemble import RandomForestClassifier

# Define the objective function to optimize
def objective(trial):
    # Define the hyperparameters to tune
    n_estimators = trial.suggest_int('n_estimators', 50, 200)
    max_depth = trial.suggest_int('max_depth', 3, 7)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)

    # Create a random forest classifier with the suggested hyperparameters
    rfc = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)

    # Train and evaluate the model on the training set
    rfc.fit(X_train, y_train)
    score = rfc.score(X_test, y_test)

    return score

# Use Optuna to search the hyperparameter space
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

# Print the best hyperparameters and score
print('Best hyperparameters: ', study.best_params)
print('Best score: ', study.best_value)
</code>"

user,"I have created an objective function, what do I do next?"
bot,"a little context would help, do you mean in optuna?"
user,"yes"
bot,"After you have defined an objective function, create a study object and pass the objective function to the optimize method:

<code>
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=30)
</code>"

#
#
#  PyTorch | Loading Data
#
#

db_pt0,"
<code>
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader

# Define a custom dataset class
class MyDataset(Dataset):
    def __init__(self, csv_file):
        self.data = pd.read_csv(csv_file)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        x = torch.tensor(row[:-1].values, dtype=torch.float32)
        y = torch.tensor(row[-1], dtype=torch.long)
        return x, y

# Load the CSV file into a dataset
dataset = MyDataset('my_data.csv')
</code>"

db_pt1,"
<code>
# Create a data loader for batching and shuffling the data
batch_size = 32
shuffle = True
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

# Iterate over the data loader to access batches of data
for batch in dataloader:
    x_batch, y_batch = batch
    print('Input batch shape:', x_batch.shape)
    print('Output batch shape:', y_batch.shape)
</code>"

#
#
#  PySpark | Functions & Data Type
#
#

db_ps0,"
You can import the following data types from <code>pyspark.sql.types</code>:

    [BinaryType] : Binary data
    [BooleanType] : Boolean values
    [ByteType] : A byte value
    [DateType] : A datetime value
    [DoubleType] : A floating-point double value
    [IntegerType] : An integer value
    [LongType] : A long integer value
    [NullType] : A null value
    [ShortType] : A short integer value
    [StringType] : A text string
    [TimestampType] : A timestamp value (typically in seconds from 1/1/1970)
    [UnknownType] : A value of unidentified type
"
db_ps1,"
<code>
from pyspark.sql.types import StructTypes

schema = StructType() \
      .add('PassengerId',IntegerType(),True) \
      .add('Name',StringType(),True) \
      .add('Fare',DoubleType(),True) \
      .add('Decommisioned',BooleanType(),True)
</code>"
db_ps2,"
<code>
from pyspark.sql.functions import col

# create a PySpark DataFrame
data = [('Alice', 25), ('Bob', 30), ('Charlie', 35)]
df = spark.createDataFrame(data, ['name', 'age'])

# use withColumn to add a new column
df_with_city = df.withColumn('city', col('name').substr(1,3))

# show the resulting DataFrame
df_with_city.show()
</code>
"
db_ps3,"
<code>
# create a DataFrame
data = [('John Doe', 25), ('Jane Smith', 30), ('Charlie Brown', 35)]
df = spark.createDataFrame(data, ['name', 'age'])

# use substr to extract a substring
df = df.withColumn('first_name', df['name'].substr(1, 4))

# show the resulting DataFrame
df.show()
</code>"
db_ps4,"
Here are some commonly used options:
- [header] : Whether the first line of the CSV file should be treated as a header row (default: False).
- [inferSchema] : Whether to infer the data types of each column from the content of the CSV file (default: False)
- [delimiter] : The delimiter used to separate fields in the CSV file (default: ,).
- [quote]: The character used to quote fields that contain special characters (default: '')
- [escape]: The character used to escape special characters within quoted fields (default: \)
- [nullValue] : The string that represents null values in the CSV file (default: empty string)
- [nanValue] : The string that represents NaN values in the CSV file (default: 'NaN')"

db_ps5,"
<code>
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

schema = StructType([
    StructField('name', StringType(), True),
    StructField('age', IntegerType(), True),
    StructField('city', StringType(), True)
])

df = spark.read.csv('data.csv', schema=schema)
</code>"
db_ps6,"
<code>
# create a DataFrame
data = [('Alice', 25), ('Bob', 30), ('Charlie', 35), ('Alice', 25)]
df = spark.createDataFrame(data, ['name', 'age'])

# use distinct to get unique rows
distinct_df = df.distinct()

# show the resulting DataFrame
distinct_df.show()
</code>"
db_ps7,"
<code>select()</code> : This function is used to select specific columns from a DataFrame
<code>filter()</code> : This function is used to filter rows based on a condition
<code>groupBy()</code> : This function is used to group the DataFrame by one or more columns
<code>agg()</code> : This function is used to perform aggregate functions on the grouped DataFrame
<code>join()</code> : This function is used to join two DataFrames based on a common column
<code>orderBy()</code> : This function is used to sort the DataFrame based on one or more columns
<code>distinct()</code> : This function is used to remove duplicate rows from the DataFrame
<code>count()</code> : This function is used to count the number of rows in the DataFrame
<code>show()</code> : This function is used to display the contents of the DataFrame in a tabular format
<code>write()</code> : This function is used to write the DataFrame to an external storage system such as HDFS or S3"

db_ps8,"
<code>avg()</code> : Returns the average value of a column
<code>sum()</code> : Returns the sum of a column
<code>min()</code> : Returns the minimum value of a column
<code>max()</code> : Returns the maximum value of a column
<code>count()</code> : Returns the number of non-null values in a column
<code>variance()</code> : Returns the variance of a column
<code>stddev()</code> : Returns the standard deviation of a column
<code>corr()</code> : Returns the correlation between two columns
<code>covar_pop()</code> : Returns the population covariance between two columns
<code>covar_samp()</code> : Returns the sample covariance between two columns"

db_ps9,"
- <code>agg</code> : Used for aggregate operations on a DataFrame or GroupedData object
- <code>col</code> : Used to reference a column in a DataFrame
- <code>count</code> : Used to count the number of non-null values in a column or DataFrame
- <code>distinct</code> : Used to return distinct values in a column or DataFrame
- <code>filter</code> : Used to filter rows in a DataFrame based on a condition
- <code>groupBy</code> : Used to group rows in a DataFrame by one or more columns
- <code>join</code> : Used to join two DataFrames based on a common column or set of columns
- <code>lit</code> : Used to create a literal value that can be added as a new column to a DataFrame
- <code>select</code> : Used to select specific columns from a DataFrame
- <code>sum</code> : Used to calculate the sum of values in a column or DataFrame"

db_ps10,"
<code>
from pyspark.sql.functions import sum,avg,stddev,variance,min,max

df.select(sum('column')).show() # column sum
df.select(avg('column')).show() # column average
df.select(stddev('column')).show() # column standard deviation
df.select(variance('column')).show() # column variance
df.select(min('column')).show() # column minimum
df.select(max('column')).show() # column maximum
</code>

db_ps11,"
# select specific columns
<code>df.select('column1', 'column2')</code>

# filter rows based on condition
<code>df.select('*').filter(df.column1 > 10)</code>

# select distinct values
<code>df.select('column1').distinct()</code>

# sorting data
<code>df.select('*').orderBy(df.column1.desc())</code>

# Limiting the number of rows returned
<code>df.select('*').limit(10)</code>"

db_ps12,"
<code>
from pyspark.sql import SparkSession
from pyspark.sql import functions as f

# create spark session
spark = SparkSession.builder\
                    .appName('name')\
                    .getOrCreate()"
</code>"

db_ps13,"
<code>
# Count the number of missing values in each column
missing_values = df.select([col(c).isNull().sum().alias(c) for c in df.columns])
missing_values.show()
</code>",

db_ps14,"
<code>
# Count the number of missing values in each row
missing_values = df.select([col(c).isNull().cast('int').alias(c) for c in df.columns])
missing_values = missing_values.withColumn('missing_count', sum(missing_values[col_name] for col_name in missing_values.columns))
</code>"

db_ps15,"
<code>
from pyspark.sql.functions import col

# Load data into a PySpark DataFrame
df = spark.read.csv('path/to/data.csv', header=True)

# Set the threshold for missing values
threshold = 10

# Count the number of missing values in each row
missing_values = df.select([col(c).isNull().cast('int').alias(c) for c in df.columns])
missing_values = missing_values.withColumn('missing_count', sum(missing_values[col_name] for col_name in missing_values.columns))

# Remove rows with missing values above the threshold
df_clean = df.filter(missing_values['missing_count'] <= threshold)
</code>
"

db_ps16,"
<code>
from pyspark.sql.functions import col

# Load data into a PySpark DataFrame
df = spark.read.csv('data.csv', header=True)

# show the number of missing values in each column
missing_values = df.select([col(c).isNull().sum().alias(c) for c in df.columns])
missing_values.show()

# Remove rows with missing values
df_clean = df.dropna()
</code>"


#
#  PySpark | Cheatsheet 
# 

db_pscs0,"
<code>
people.createOrReplaceTempView('people')
spark.sql(
    '''
    SELECT *
    FROM people
        INNER JOIN places
            ON people.city = LOWER(places.location)
    '''
).show() 
</code>"

db_pscs1,"
df = df_1.join(df_2, on=['key'], how='inner') 
df = df_1.join(df_2, df_1.key < df_2.key, how='inner') # inner w/ condition
df = df_1.join(df_2, on=['key'], how='outer')
df = df_1.join(df_2, on=['key'], how='left')
df = df_1.join(df_2, on=['key'], how='right')
df = df_1.join(df_2, on=['key'], how='left_semi')
df = df_1.join(df_2, on=['key], how='left_anti')"

db_pscs2,"
<code>
df.sort('goals', ascending=True).collect()
df.sort(df.goals.asc()).collect()
df.orderBy(['goals'], ascending = [0,1]).collect()
</code>"

db_pscs3,"
<code>
df.groupby('players').count().show()
df.groupby('players').agg(spark_max('goals'), spark_min('goals'), spark_sum('goals').alias('total_goal_num')).show()
</code>"

db_pscs4,"
<code>
# Select using a when-otherwise clause:
df.select('goals', f.when(df.goals == 0, 'boring').otherwise('interesting'))

# Select using like:
df.select('sport', df.sport.like('basketball'))

# Select using between:
df.select(df.goals.between(1, 3))

# Select using startswith or endswith:
df.select('sports', df.players.startwith('B'))
df.select(df.players.endswith('s'))

# select a substring
df.select(df.players.substr(1, 4).alias('nickname'))
</code>"

db_pscs5,"
<code>
# Select a single column
df.select('basketball')

# Select multiple columns:
df.select('basketball', 'football')

# Select a filtered version of a column:
df.select(df['goals'] >= 2)

# Select a modified version of a column:
df.select(df['goals'] + 1)
</code>"

db_pscs6,"
<code>
import pyspark.sql.functions as f

# Add columns with Spark native functions
new_df = df.withColumn('column_3_multiplied', 3 * f.col('column_3_original'))
</code>"

db_pscs7,"
<code>
import pyspark.sql.functions as f
from psyspark.sql.types import *

# user defined function
def example_func(filter_value):
    if values >= 5:
        return 'enough free spots'
    else:
        return 'not enough free spots'

my_udf = f.udf(example_func, StringType())

cinema_tickets = cinema.withColumn('free_spots', my_udf('spots') )
</code>"

db_pscs8,"
<code>
population = [ ('Croatia', 4_058_000), ('Oregon', 4_218_000 ) ]

cols = ['state', 'population']

df = spark.createDataFrame(data=population, schema=cols)
</code>"

db_pscs9,"
<code>
population = [ ('Croatia', 4_058_000), ('Oregon', 4_218_000 ) ]

pop_schema = StructType([
    StructField('state', StringType(), True),   
    StructField('population', IntegerType(), True)])

df = spark.createDataFrame(data=population, schema=pop_schema)
</code>"


