{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\n\n\n'''\n\nEmbedding Layer Usage \n\n'''\n\n# The Embedding has a vocabulary of 50 and an input length of 4\n# We will choose a small embedding space of 8 dimensions\n# Output from the Embedding layer will be 4 vectors of 4 dimensions each, one for each word. \n# We flatten this to a one 32-element vector to pass on to the Dense output layer.\n\n# define documents\ndocs = ['Well done!','Good work','Great effort','nice work','Excellent!',\n'Weak','Poor effort!','not good','poor work','Could have done better.']\n\n# define class labels\nlabels = array([1,1,1,1,1,0,0,0,0,0])\n\n# integer encode the documents\nprint('One-Hot-Encoded:\\n')\nvocab_size = 20\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\nprint(encoded_docs)\n\n# pad documents to a max length of 4 words\nmax_length = 4\npadded_docs = pad_sequences(encoded_docs, \n                            maxlen=max_length, \n                            padding='post')\nprint('\\nPadded Documents\\n')\nprint(padded_docs)\n\n# define the model\nprint('\\nBinary Classification Model\\n')\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 8, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy', \n              metrics=['accuracy'])\nprint(model.summary())\n\n# # fit the model\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n\nprint('\\nEvaluation\\n')\nprint('Accuracy: %f' % (accuracy*100))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-18T07:54:44.683735Z","iopub.execute_input":"2022-12-18T07:54:44.684171Z","iopub.status.idle":"2022-12-18T07:54:45.587357Z","shell.execute_reply.started":"2022-12-18T07:54:44.684136Z","shell.execute_reply":"2022-12-18T07:54:45.586544Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"One-Hot-Encoded:\n\n[[18, 18], [11, 10], [1, 10], [17, 10], [4], [4], [11, 10], [2, 11], [11, 10], [18, 10, 18, 10]]\n\nPadded Documents\n\n[[18 18  0  0]\n [11 10  0  0]\n [ 1 10  0  0]\n [17 10  0  0]\n [ 4  0  0  0]\n [ 4  0  0  0]\n [11 10  0  0]\n [ 2 11  0  0]\n [11 10  0  0]\n [18 10 18 10]]\n\nBinary Classification Model\n\nModel: \"sequential_10\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_10 (Embedding)     (None, 4, 8)              160       \n_________________________________________________________________\nflatten_10 (Flatten)         (None, 32)                0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 1)                 33        \n=================================================================\nTotal params: 193\nTrainable params: 193\nNon-trainable params: 0\n_________________________________________________________________\nNone\n\nEvaluation\n\nAccuracy: 69.999999\n","output_type":"stream"}]}]}