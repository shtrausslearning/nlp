{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8d0c16",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-12-28T18:24:10.917758Z",
     "iopub.status.busy": "2022-12-28T18:24:10.917184Z",
     "iopub.status.idle": "2022-12-28T18:24:24.199822Z",
     "shell.execute_reply": "2022-12-28T18:24:24.198109Z"
    },
    "papermill": {
     "duration": 13.300095,
     "end_time": "2022-12-28T18:24:24.203012",
     "exception": false,
     "start_time": "2022-12-28T18:24:10.902917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertviz\r\n",
      "  Downloading bertviz-1.4.0-py3-none-any.whl (157 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m735.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from bertviz) (1.24.93)\r\n",
      "Requirement already satisfied: torch>=1.0 in /opt/conda/lib/python3.7/site-packages (from bertviz) (1.11.0+cpu)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from bertviz) (4.64.0)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from bertviz) (0.1.97)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from bertviz) (2021.11.10)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from bertviz) (2.28.1)\r\n",
      "Requirement already satisfied: transformers>=2.0 in /opt/conda/lib/python3.7/site-packages (from bertviz) (4.20.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.0->bertviz) (4.4.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (0.10.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (3.7.1)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (0.12.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (21.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (1.21.6)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (4.13.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (6.0)\r\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.93 in /opt/conda/lib/python3.7/site-packages (from boto3->bertviz) (1.27.93)\r\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->bertviz) (0.6.0)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->bertviz) (1.0.1)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->bertviz) (2.1.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->bertviz) (1.26.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->bertviz) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->bertviz) (2022.9.24)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.93->boto3->bertviz) (2.8.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers>=2.0->bertviz) (3.0.9)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=2.0->bertviz) (3.8.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.93->boto3->bertviz) (1.15.0)\r\n",
      "Installing collected packages: bertviz\r\n",
      "Successfully installed bertviz-1.4.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50010e",
   "metadata": {
    "papermill": {
     "duration": 0.011913,
     "end_time": "2022-12-28T18:24:24.226546",
     "exception": false,
     "start_time": "2022-12-28T18:24:24.214633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <b>1 <span style='color:#F1A424'>|</span> Creating a Transformer</b> \n",
    "\n",
    "***\n",
    "\n",
    "In the following notebook, we'll look at the following things\n",
    "\n",
    "<div style=\" background-color:#3b3745; padding: 13px 13px; border-radius: 8px; color: white\">\n",
    "    \n",
    "<ul>\n",
    "<li>Simple Attention</li>\n",
    "<li>Multi-Head Self Attention</li>\n",
    "<li>Feed Forward Layer</li>\n",
    "<li>Normalisation</li>\n",
    "<li>Position Embeddings</li>\n",
    "<li>Transformer Encoder</li>\n",
    "<li>Classifier Head</li>\n",
    "</ul> \n",
    "</div> \n",
    "\n",
    "<br>\n",
    "\n",
    "## <b>2 <span style='color:#F1A424'>|</span> Simple Self-Attention</b> \n",
    "\n",
    "***\n",
    "\n",
    "### <b><span style='color:#F1A424'>Types of Attention</span></b>\n",
    "\n",
    "**<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention</mark>**\n",
    " \n",
    "- Mechanism which allows networks to assign **different weight distributions to each element** in a sequence \n",
    "- Elements in sequence - `token embeddings` (each token mapped to a vector of fixed dimension) (eg. BERT model - 768 dimensions)\n",
    " \n",
    " \n",
    "**<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">self-attention</mark>**\n",
    "\n",
    "- Instead of using fixed embeddings for each token, can use whole sequence to **compute weighted average** of each `embedding`\n",
    "- One can think of self-attention as a form of averaging\n",
    "- Common form of `self-attention` **scaled dot-product attention** \n",
    "\n",
    "\n",
    "### <b><span style='color:#F1A424'>Four Main Steps</span></b>\n",
    "\n",
    "- Project each `token embedding` into three vectors **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">query</mark>**,**<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">key</mark>**,**<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">value</mark>**\n",
    "- Compute **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention scores</mark>** (nxn)\n",
    "\n",
    "    - (we determine how much the query & key vectors relate to eachother using a similarity function)\n",
    "    - Similarity function for scaled dot-product attention - dot product\n",
    "    - queries & keys that are similar will have large dot product & visa versa\n",
    "    - Outputs from this step - attention scores\n",
    "    \n",
    "    \n",
    "- Compute **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention weight</mark>** (wij)\n",
    "\n",
    "    - dot products produce large numbers \n",
    "    - attention scores first multiplied by a scaling factor to normalise their variance\n",
    "    - Then normalised with softmax to ensure all column values sum to 1\n",
    "    \n",
    "    \n",
    "- Update the token embeddings \n",
    "\n",
    "    - multiply the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">weights</mark>** by the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">value</mark>** vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497b0597",
   "metadata": {
    "papermill": {
     "duration": 0.010222,
     "end_time": "2022-12-28T18:24:24.247347",
     "exception": false,
     "start_time": "2022-12-28T18:24:24.237125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf44a6",
   "metadata": {
    "papermill": {
     "duration": 0.010009,
     "end_time": "2022-12-28T18:24:24.267818",
     "exception": false,
     "start_time": "2022-12-28T18:24:24.257809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <b><span style='color:#F1A424'>Step by Step</span></b>\n",
    "\n",
    "#### 1. Document tokenisation\n",
    "\n",
    "- Each token in the sentence has been mapped to a unique identifier from a vocabulary \n",
    "- We start off by using the `bert-base-uncased` pretrained tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4050243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:24:24.292176Z",
     "iopub.status.busy": "2022-12-28T18:24:24.290913Z",
     "iopub.status.idle": "2022-12-28T18:24:54.131440Z",
     "shell.execute_reply": "2022-12-28T18:24:54.129972Z"
    },
    "papermill": {
     "duration": 29.857322,
     "end_time": "2022-12-28T18:24:54.135596",
     "exception": false,
     "start_time": "2022-12-28T18:24:24.278274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4073cd6f5976482d84a836ba30dfde7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed44b1c23dde4812b5b601a763c51fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9321701727424b899f550220dab1793a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf2d0efbc654a6292c3a833b93e728e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 433/433 [00:00<00:00, 229280.85B/s]\n",
      "100%|██████████| 440473133/440473133 [00:15<00:00, 28687024.42B/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from bertviz.transformers_neuron_view import BertModel\n",
    "# from bertviz.neuron_view import show\n",
    "\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = BertModel.from_pretrained(model_ckpt)\n",
    "\n",
    "# document\n",
    "text = \"time flies like an arrow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f739fb58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:24:54.181334Z",
     "iopub.status.busy": "2022-12-28T18:24:54.179893Z",
     "iopub.status.idle": "2022-12-28T18:24:54.203605Z",
     "shell.execute_reply": "2022-12-28T18:24:54.202323Z"
    },
    "papermill": {
     "duration": 0.047114,
     "end_time": "2022-12-28T18:24:54.206200",
     "exception": false,
     "start_time": "2022-12-28T18:24:54.159086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2051, 10029,  2066,  2019,  8612]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenise input (text)\n",
    "inputs = tokenizer(text, \n",
    "                   return_tensors=\"pt\",      # pytorc tensor\n",
    "                   add_special_tokens=False) # don't use pad, sep tokens\n",
    "\n",
    "print(inputs.input_ids)\n",
    "inputs.input_ids.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee2a113",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:24:54.251043Z",
     "iopub.status.busy": "2022-12-28T18:24:54.249801Z",
     "iopub.status.idle": "2022-12-28T18:24:54.259305Z",
     "shell.execute_reply": "2022-12-28T18:24:54.257293Z"
    },
    "papermill": {
     "duration": 0.034843,
     "end_time": "2022-12-28T18:24:54.263323",
     "exception": false,
     "start_time": "2022-12-28T18:24:54.228480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2051, 10029, 2066, 2019, 8612]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor to list\n",
    "inputs['input_ids'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356bdae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:24:54.307078Z",
     "iopub.status.busy": "2022-12-28T18:24:54.306590Z",
     "iopub.status.idle": "2022-12-28T18:25:03.275313Z",
     "shell.execute_reply": "2022-12-28T18:25:03.273382Z"
    },
    "papermill": {
     "duration": 8.993368,
     "end_time": "2022-12-28T18:25:03.277472",
     "exception": false,
     "start_time": "2022-12-28T18:24:54.284104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time flies like an arrow'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode sequence\n",
    "tokenizer.decode(inputs['input_ids'].tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f0e42",
   "metadata": {
    "papermill": {
     "duration": 0.021676,
     "end_time": "2022-12-28T18:25:03.319739",
     "exception": false,
     "start_time": "2022-12-28T18:25:03.298063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "At this point:\n",
    "\n",
    "- `inputs.inpits_ids` A tensor of id mapped tokens\n",
    "- Token embeddings are **independent of their context**\n",
    "- **Homonyms** (same spelling, but different meaning) have the same representation\n",
    "\n",
    "Role of subsequent attention layers:\n",
    "\n",
    "- Mix the **token embeddings** to disambiguate & inform the representation of each token with the context of its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7691136b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:03.365337Z",
     "iopub.status.busy": "2022-12-28T18:25:03.364388Z",
     "iopub.status.idle": "2022-12-28T18:25:03.992980Z",
     "shell.execute_reply": "2022-12-28T18:25:03.990466Z"
    },
    "papermill": {
     "duration": 0.654778,
     "end_time": "2022-12-28T18:25:03.995662",
     "exception": false,
     "start_time": "2022-12-28T18:25:03.340884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 hidden size\n",
      "30522 vocabulary size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "\n",
    "print(config.hidden_size,\"hidden size\")\n",
    "print(config.vocab_size,\"vocabulary size\")\n",
    "\n",
    "# load sample embedding layer of size (30522,758) -> same as bert-base\n",
    "token_emb = nn.Embedding(config.vocab_size,\n",
    "                         config.hidden_size)\n",
    "token_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5610345",
   "metadata": {
    "papermill": {
     "duration": 0.019948,
     "end_time": "2022-12-28T18:25:04.035890",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.015942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. Embedding Vectors\n",
    "\n",
    "- Convert Tokenised data into embedding data (768 dimensions) using vocab of 30522 tokens\n",
    "- Each input_ids is **mapped to one of 30522 embedding vectors** stored in nn.embedding, each with a size of 768 \n",
    "- Our output will be [batch_size,seq_len,hidden_dim] by calling `nn.Embedding(hidden)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df58f94a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:04.077086Z",
     "iopub.status.busy": "2022-12-28T18:25:04.076715Z",
     "iopub.status.idle": "2022-12-28T18:25:04.091248Z",
     "shell.execute_reply": "2022-12-28T18:25:04.090208Z"
    },
    "papermill": {
     "duration": 0.037926,
     "end_time": "2022-12-28T18:25:04.093241",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.055315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "inputs_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc332a8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:04.135810Z",
     "iopub.status.busy": "2022-12-28T18:25:04.135396Z",
     "iopub.status.idle": "2022-12-28T18:25:04.146599Z",
     "shell.execute_reply": "2022-12-28T18:25:04.145509Z"
    },
    "papermill": {
     "duration": 0.035673,
     "end_time": "2022-12-28T18:25:04.148875",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.113202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2257, -0.4307,  1.8177,  ...,  0.7229,  3.2706, -0.7792],\n",
       "         [ 0.4286,  0.0174, -0.7029,  ..., -0.8039, -0.6916,  0.6626],\n",
       "         [-0.3575, -0.6561,  0.5093,  ..., -1.2935,  0.0352,  1.3152],\n",
       "         [-0.4761,  0.8055,  0.4843,  ...,  1.2622, -0.6694, -0.1447],\n",
       "         [ 0.7571,  0.1967,  0.0324,  ...,  0.1879,  1.6343, -1.1001]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 embedding vectors of 768 dimensions\n",
    "inputs_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450a65a",
   "metadata": {
    "papermill": {
     "duration": 0.02058,
     "end_time": "2022-12-28T18:25:04.189971",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.169391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 3. Create query, key and value vectors\n",
    "\n",
    "- As the most simplistic case of attention, **we set them equal to one another**\n",
    "- attention mechanism with equal query and key vectors will assign a **very large score to identical words in the context** (diagonal component of matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "501c827d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:04.233739Z",
     "iopub.status.busy": "2022-12-28T18:25:04.233167Z",
     "iopub.status.idle": "2022-12-28T18:25:04.244293Z",
     "shell.execute_reply": "2022-12-28T18:25:04.241989Z"
    },
    "papermill": {
     "duration": 0.036219,
     "end_time": "2022-12-28T18:25:04.247007",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.210788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query and key components\n",
      "\n",
      "query size: torch.Size([1, 5, 768])\n",
      "key size: torch.Size([1, 768, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "\n",
    "# setting them equal to one another\n",
    "print(\"query and key components\\n\")\n",
    "query = key = value = inputs_embeds\n",
    "print('query size:',query.size())\n",
    "dim_k = key.size(-1)   # hidden dimension \n",
    "print('key size:',key.transpose(1,2).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a8f8d",
   "metadata": {
    "papermill": {
     "duration": 0.02195,
     "end_time": "2022-12-28T18:25:04.293063",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.271113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 4. Compute the attention scores\n",
    "\n",
    "- Compute **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention scores</mark>** using the **dot product as the similarity function**\n",
    "- `torch.bmm` - batch matrix matrix product (as we work in batches during training)\n",
    "- If we need to transpose a vector `vector.transpose(1,2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fdbc82b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:04.337006Z",
     "iopub.status.busy": "2022-12-28T18:25:04.336380Z",
     "iopub.status.idle": "2022-12-28T18:25:04.357684Z",
     "shell.execute_reply": "2022-12-28T18:25:04.356359Z"
    },
    "papermill": {
     "duration": 0.046603,
     "end_time": "2022-12-28T18:25:04.360229",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.313626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dot product (attention scores)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product & apply normalisation\n",
    "print(\"\\ndot product (attention scores)\")\n",
    "scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb7269a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:04.404232Z",
     "iopub.status.busy": "2022-12-28T18:25:04.403850Z",
     "iopub.status.idle": "2022-12-28T18:25:04.412142Z",
     "shell.execute_reply": "2022-12-28T18:25:04.410591Z"
    },
    "papermill": {
     "duration": 0.034603,
     "end_time": "2022-12-28T18:25:04.414951",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.380348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[25.8117, -0.1492,  0.4068,  0.6981,  0.0988],\n",
       "         [-0.1492, 30.5535, -0.3039, -0.4122, -0.1059],\n",
       "         [ 0.4068, -0.3039, 27.0213,  0.5284,  0.3396],\n",
       "         [ 0.6981, -0.4122,  0.5284, 29.3416,  1.0719],\n",
       "         [ 0.0988, -0.1059,  0.3396,  1.0719, 27.9608]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention scores\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59e5c39",
   "metadata": {
    "papermill": {
     "duration": 0.020504,
     "end_time": "2022-12-28T18:25:04.455471",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.434967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 5. Compute attention weights (softmax function)\n",
    "\n",
    "- Created a 5x5 matrix of **attention scores** per sample in the batch\n",
    "- Apply the softmax for normalisation to get the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention weights</mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "210b7a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:04.498208Z",
     "iopub.status.busy": "2022-12-28T18:25:04.497738Z",
     "iopub.status.idle": "2022-12-28T18:25:04.509230Z",
     "shell.execute_reply": "2022-12-28T18:25:04.507256Z"
    },
    "papermill": {
     "duration": 0.036296,
     "end_time": "2022-12-28T18:25:04.512437",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.476141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sotfmax applied, attention weights :\n",
      "\n",
      "torch.Size([1, 5, 5])\n",
      "tensor([[[1.0000e+00, 5.3127e-12, 9.2636e-12, 1.2397e-11, 6.8080e-12],\n",
      "         [4.6343e-14, 1.0000e+00, 3.9701e-14, 3.5627e-14, 4.8393e-14],\n",
      "         [2.7635e-12, 1.3577e-12, 1.0000e+00, 3.1207e-12, 2.5840e-12],\n",
      "         [3.6332e-13, 1.1970e-13, 3.0659e-13, 1.0000e+00, 5.2800e-13],\n",
      "         [7.9376e-13, 6.4682e-13, 1.0099e-12, 2.1005e-12, 1.0000e+00]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "sum of column values:/n\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"sotfmax applied, attention weights :\\n\")\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "print(weights.size())\n",
    "print(weights)\n",
    "\n",
    "print(\"\\nsum of column values:/n\")\n",
    "weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f34cb",
   "metadata": {
    "papermill": {
     "duration": 0.021875,
     "end_time": "2022-12-28T18:25:04.555783",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.533908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 6. Update values \n",
    "\n",
    "Multiply the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention weights</mark>** matrix by the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">values</mark>** vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3804a9ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:04.600509Z",
     "iopub.status.busy": "2022-12-28T18:25:04.600007Z",
     "iopub.status.idle": "2022-12-28T18:25:04.610717Z",
     "shell.execute_reply": "2022-12-28T18:25:04.609427Z"
    },
    "papermill": {
     "duration": 0.035878,
     "end_time": "2022-12-28T18:25:04.612847",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.576969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2257, -0.4307,  1.8177,  ...,  0.7229,  3.2706, -0.7792],\n",
      "         [ 0.4286,  0.0174, -0.7029,  ..., -0.8039, -0.6916,  0.6626],\n",
      "         [-0.3575, -0.6561,  0.5093,  ..., -1.2935,  0.0352,  1.3152],\n",
      "         [-0.4761,  0.8055,  0.4843,  ...,  1.2622, -0.6694, -0.1447],\n",
      "         [ 0.7571,  0.1967,  0.0324,  ...,  0.1879,  1.6343, -1.1001]]],\n",
      "       grad_fn=<BmmBackward0>)\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "attn_outputs = torch.bmm(weights, value)\n",
    "print(attn_outputs)\n",
    "print(attn_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d0eb18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:04.655548Z",
     "iopub.status.busy": "2022-12-28T18:25:04.655170Z",
     "iopub.status.idle": "2022-12-28T18:25:04.662499Z",
     "shell.execute_reply": "2022-12-28T18:25:04.660979Z"
    },
    "papermill": {
     "duration": 0.032234,
     "end_time": "2022-12-28T18:25:04.665174",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.632940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Scalar Dot Product Attention\n",
    "scores = query*key.T / sqrt(dims)\n",
    "weight = softmax(scores) \n",
    "\n",
    "'''\n",
    "\n",
    "def sdp_attention(query, key, value):\n",
    "    dim_k = query.size(-1) # dimension component\n",
    "    sfact = sqrt(dim_k)     \n",
    "    scores = torch.bmm(query, key.transpose(1,2)) / sfact\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3b75a",
   "metadata": {
    "papermill": {
     "duration": 0.021,
     "end_time": "2022-12-28T18:25:04.707261",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.686261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <b>3 <span style='color:#F1A424'>|</span> Multi-Head Self Attention</b> \n",
    "\n",
    "***\n",
    "\n",
    "- The meaning of the word will be better informed by **complementary words in the context** than by **identical words** (which gives 1)\n",
    "\n",
    "### <b><span style='color:#F1A424'>Simplistic Approach</span></b>\n",
    "\n",
    "- We only used the embeddings \"as is\" to compute the **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention scores</mark>** **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention weights</mark>**\n",
    "\n",
    "### <b><span style='color:#F1A424'>Better Approach</span></b>\n",
    "\n",
    "- The **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">self-attention</mark>** layer applies **three independent linear transformations to each embedding** to generate **query**, **key** & **value**\n",
    "- These transformations project the embeddings and **each projection carries its own set of learnable parameters**, \n",
    "- This **allows the self-attention layer to focus on different semantic aspects of the sequence**\n",
    "\n",
    "\n",
    "\n",
    "Its beneficial to have **multiple sets of linear projections** (each one represents an **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention head</mark>**)\n",
    "\n",
    "- Why do we need more than one **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">attention head</mark>**?\n",
    "- The **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">softmax</mark>** of one head tends to focus on mostly **one aspect of similarity**\n",
    "- **Several heads** allows the model to **focus on several apsects at once**\n",
    "- Eg. one head can focus on subject-verb interaction, another finds nearby adjectives\n",
    "- **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">CV analogy</mark>**: filters; one filter responsible for detecting the head, another for facial features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e85fd653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:04.755897Z",
     "iopub.status.busy": "2022-12-28T18:25:04.755354Z",
     "iopub.status.idle": "2022-12-28T18:25:04.765302Z",
     "shell.execute_reply": "2022-12-28T18:25:04.762866Z"
    },
    "papermill": {
     "duration": 0.038004,
     "end_time": "2022-12-28T18:25:04.768445",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.730441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nn.linear : apply linear transformation to incoming data\n",
    "#             y = x * A^T + b\n",
    "# Ax = b where x is input, b is output, A is weight\n",
    "\n",
    "# calculate scaled dot product attention matrix\n",
    "# Requires embedding dimension \n",
    "# Each attention head is made of different q,k,v vectors\n",
    "\n",
    "class att(nn.Module):\n",
    "    \n",
    "    # initalisation \n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the three vectors\n",
    "        # input - embed_dim, output - head_dim\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    # main class operation\n",
    "    def forward(self, hidden_state):\n",
    "        \n",
    "        # calculate scaled dot product given a \n",
    "        attn_outputs = sdp_attention(\n",
    "            self.q(hidden_state), \n",
    "            self.k(hidden_state), \n",
    "            self.v(hidden_state))\n",
    "        \n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74da173e",
   "metadata": {
    "papermill": {
     "duration": 0.021746,
     "end_time": "2022-12-28T18:25:04.812965",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.791219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`AttentionHead` will be used in the construction of a model\n",
    "\n",
    "- We’ve **initialised three independent linear layers** that apply matrix multiplication to the embedding vectors to produce tensors of shape [batch_size, seq_len, head_dim]\n",
    "- Where head_dim is the number of dimensions we are projecting into\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea45f698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:04.860100Z",
     "iopub.status.busy": "2022-12-28T18:25:04.859599Z",
     "iopub.status.idle": "2022-12-28T18:25:04.865528Z",
     "shell.execute_reply": "2022-12-28T18:25:04.864459Z"
    },
    "papermill": {
     "duration": 0.033431,
     "end_time": "2022-12-28T18:25:04.868783",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.835352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 heads\n",
      "768 hidden state embedding dimension\n"
     ]
    }
   ],
   "source": [
    "print(config.num_attention_heads,'heads')\n",
    "print(config.hidden_size,'hidden state embedding dimension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f3959cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:04.915152Z",
     "iopub.status.busy": "2022-12-28T18:25:04.914685Z",
     "iopub.status.idle": "2022-12-28T18:25:04.923917Z",
     "shell.execute_reply": "2022-12-28T18:25:04.922324Z"
    },
    "papermill": {
     "duration": 0.036122,
     "end_time": "2022-12-28T18:25:04.927495",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.891373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "att(\n",
       "  (q): Linear(in_features=768, out_features=12, bias=True)\n",
       "  (k): Linear(in_features=768, out_features=12, bias=True)\n",
       "  (v): Linear(in_features=768, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = config.hidden_size\n",
    "num_heads = config.num_attention_heads\n",
    "\n",
    "# Initialised just one head, requires token embedding vector for forward operation\n",
    "attention_outputs = att(embed_dim,num_heads)\n",
    "attention_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74276529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:04.977566Z",
     "iopub.status.busy": "2022-12-28T18:25:04.977115Z",
     "iopub.status.idle": "2022-12-28T18:25:04.987269Z",
     "shell.execute_reply": "2022-12-28T18:25:04.985255Z"
    },
    "papermill": {
     "duration": 0.038386,
     "end_time": "2022-12-28T18:25:04.990584",
     "exception": false,
     "start_time": "2022-12-28T18:25:04.952198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class matt(nn.Module):\n",
    "    \n",
    "    # Config during initalisation\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # model params, read from config file\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # attention head (define only w/o hidden state)\n",
    "        # each attention head is initialised with embedd/heads \n",
    "        self.heads = nn.ModuleList(\n",
    "            [att(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "        \n",
    "        # output uses whole embedding dimension for output\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    # Apply operation for multihead attention, requires hidden state (embeddings)\n",
    "        \n",
    "    def forward(self, hidden_state):\n",
    "        \n",
    "        # for each head embed_size/heads, calculate attention\n",
    "        heads = [head(hidden_state) for head in self.heads] \n",
    "        x = torch.cat(heads, dim=-1) # merge them together\n",
    "    \n",
    "        # apply linear transformation to multihead attension scalar product\n",
    "        x = self.output_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17d25a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:05.042188Z",
     "iopub.status.busy": "2022-12-28T18:25:05.041584Z",
     "iopub.status.idle": "2022-12-28T18:25:05.051153Z",
     "shell.execute_reply": "2022-12-28T18:25:05.048993Z"
    },
    "papermill": {
     "duration": 0.040641,
     "end_time": "2022-12-28T18:25:05.054899",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.014258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2257, -0.4307,  1.8177,  ...,  0.7229,  3.2706, -0.7792],\n",
      "         [ 0.4286,  0.0174, -0.7029,  ..., -0.8039, -0.6916,  0.6626],\n",
      "         [-0.3575, -0.6561,  0.5093,  ..., -1.2935,  0.0352,  1.3152],\n",
      "         [-0.4761,  0.8055,  0.4843,  ...,  1.2622, -0.6694, -0.1447],\n",
      "         [ 0.7571,  0.1967,  0.0324,  ...,  0.1879,  1.6343, -1.1001]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "# Input into MultiHeadAttention (token embeddings for each token)\n",
    "print(inputs_embeds)\n",
    "print(inputs_embeds.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e2f66bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:05.110953Z",
     "iopub.status.busy": "2022-12-28T18:25:05.110302Z",
     "iopub.status.idle": "2022-12-28T18:25:05.170460Z",
     "shell.execute_reply": "2022-12-28T18:25:05.168534Z"
    },
    "papermill": {
     "duration": 0.093376,
     "end_time": "2022-12-28T18:25:05.174816",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.081440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2287,  0.0604,  0.0362,  ...,  0.1805, -0.0274, -0.0454],\n",
      "         [-0.2188,  0.0154,  0.0420,  ...,  0.1469, -0.0010, -0.0786],\n",
      "         [-0.2651,  0.0482,  0.0657,  ...,  0.1920, -0.0163, -0.0702],\n",
      "         [-0.3076,  0.0556,  0.0631,  ...,  0.2168, -0.0448, -0.0093],\n",
      "         [-0.2418, -0.0258,  0.0101,  ...,  0.2184,  0.0147, -0.0526]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "# Every time will be different due to randomised weights\n",
    "multihead_attn = matt(config) # initialisation with config\n",
    "attn_output = multihead_attn(inputs_embeds) # forward by inputting embedding vectors (one for each token)\n",
    "\n",
    "# Attention output (attention weights matrix x vector weights concat)\n",
    "print(attn_output)\n",
    "print(attn_output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d45db",
   "metadata": {
    "papermill": {
     "duration": 0.021089,
     "end_time": "2022-12-28T18:25:05.217871",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.196782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <b>4 <span style='color:#F1A424'>|</span> Feed-Forward Layer</b> \n",
    "\n",
    "***\n",
    "\n",
    "Position feed-forward network \n",
    "\n",
    "Often referred to as a **position-wise feed-forward layer**\n",
    "\n",
    "- The **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">feed-forward</mark>** sublayer in the encoder & decoder -> simple **two layer fully connected neural network**\n",
    "- However, instead of processing the whole sequence of embedding as a single vector, it **processes each embedding** independently\n",
    "- Also see it referred to as a Conv1D with a kernel size of 1 (people with a CV background)\n",
    "\n",
    "A rule of thumb \n",
    "\n",
    "The **hidden size** of the **1st layer = 4x size of the embeddings** & **GELU activation function**\n",
    "- Place where most of the capacity & memorization is hypothesized to happen (most often scaled when scaling up the models)\n",
    "\n",
    "\n",
    "A feed-forward layer (eg. nn.Linear usually applied to a tensor of shape (batch_size, input_dim), where it acts on each element of the batch dimension independently\n",
    "- This is actually true for any dimension except the last one, so when we pass a tensor of shape (batch_size, seq_len, hidden_dim) the layer is applied to all token embeddings of the batch and sequence independently, which is exactly what we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea202ac6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:05.263090Z",
     "iopub.status.busy": "2022-12-28T18:25:05.262678Z",
     "iopub.status.idle": "2022-12-28T18:25:05.270590Z",
     "shell.execute_reply": "2022-12-28T18:25:05.269371Z"
    },
    "papermill": {
     "duration": 0.032823,
     "end_time": "2022-12-28T18:25:05.272828",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.240005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ff(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    # define layer operations input x\n",
    "        \n",
    "    def forward(self, x):    # note must be forward\n",
    "        x = self.gelu(self.linear_1(x))\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "102e8beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:05.319912Z",
     "iopub.status.busy": "2022-12-28T18:25:05.318683Z",
     "iopub.status.idle": "2022-12-28T18:25:05.356393Z",
     "shell.execute_reply": "2022-12-28T18:25:05.355404Z"
    },
    "papermill": {
     "duration": 0.064233,
     "end_time": "2022-12-28T18:25:05.358675",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.294442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ff(\n",
       "  (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (gelu): GELU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example for bert model\n",
    "ff(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2418d72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:05.406821Z",
     "iopub.status.busy": "2022-12-28T18:25:05.406146Z",
     "iopub.status.idle": "2022-12-28T18:25:05.416599Z",
     "shell.execute_reply": "2022-12-28T18:25:05.413625Z"
    },
    "papermill": {
     "duration": 0.039984,
     "end_time": "2022-12-28T18:25:05.421090",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.381106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "# Attention outputs\n",
    "attn_outputs\n",
    "print(attn_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69e29699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:05.471724Z",
     "iopub.status.busy": "2022-12-28T18:25:05.470321Z",
     "iopub.status.idle": "2022-12-28T18:25:05.540142Z",
     "shell.execute_reply": "2022-12-28T18:25:05.538468Z"
    },
    "papermill": {
     "duration": 0.098561,
     "end_time": "2022-12-28T18:25:05.543055",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.444494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requires config & attn_outputs outputs\n",
    "\n",
    "feed_forward = ff(config)     # initialise \n",
    "ff_outputs = feed_forward(attn_output) # forward\n",
    "ff_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b8478",
   "metadata": {
    "papermill": {
     "duration": 0.021398,
     "end_time": "2022-12-28T18:25:05.585562",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.564164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <b>5 <span style='color:#F1A424'>|</span> Normalisation Layers</b> \n",
    "\n",
    "***\n",
    "\n",
    "Transformer architecture uses **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">layer normalisation</mark>** & **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">skip connections</mark>**\n",
    "- **normalisation** - normalises batch input to have zero mean & unit variance\n",
    "- **skip connections** - pass a tensor to the next level of the model w/o processing & adding it to the processed tensor\n",
    "\n",
    "Two main approaches, when it comes to normalisation layer placement in decoder, encoder:\n",
    "- **post layer** normalisation (transformer paper, layer normalisation b/w skip connections)\n",
    "- **pre layer** normalisation \n",
    "\n",
    "<br>\n",
    "\n",
    "| `post-layer` normalisation |  `pre-layer` normalisation in literature |\n",
    "| - | - |\n",
    "| Arrangement is tricky to train from scractch, as the gradients can diverge |  Most often found arrangement\n",
    "| Used with LR warm up (learning rate gradually increased, from small value to some maximum value during training) | Places layer normalization within the span of the skip connection |\n",
    "|  | Tends to be much more stable during training, and it does not usually require any learning rate warm-up |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0b9ecd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:05.630465Z",
     "iopub.status.busy": "2022-12-28T18:25:05.629959Z",
     "iopub.status.idle": "2022-12-28T18:25:05.639520Z",
     "shell.execute_reply": "2022-12-28T18:25:05.637324Z"
    },
    "papermill": {
     "duration": 0.037112,
     "end_time": "2022-12-28T18:25:05.643509",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.606397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class encoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = matt(config)    # multihead attention layer \n",
    "        self.feed_forward = ff(config)        # feed forward layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Apply layer norm. to hidden state, copy input into query, key, value\n",
    "        # Apply attention with a skip connection\n",
    "        x = x + self.attention(self.layer_norm_1(x))\n",
    "        \n",
    "        # Apply feed-forward layer with a skip connection\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bb3875a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:05.690335Z",
     "iopub.status.busy": "2022-12-28T18:25:05.689359Z",
     "iopub.status.idle": "2022-12-28T18:25:05.760542Z",
     "shell.execute_reply": "2022-12-28T18:25:05.757848Z"
    },
    "papermill": {
     "duration": 0.096752,
     "end_time": "2022-12-28T18:25:05.763167",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.666415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input torch.Size([1, 5, 768])\n",
      "output torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "# Transformer layer output\n",
    "encoder_layer = encoderLayer(config)\n",
    "\n",
    "print('input',inputs_embeds.shape) \n",
    "print('output',encoder_layer(inputs_embeds).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3941d33",
   "metadata": {
    "papermill": {
     "duration": 0.020999,
     "end_time": "2022-12-28T18:25:05.805378",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.784379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We’ve now implemented our transformer encoder layer\n",
    "- However, there is an issue with the way we set up the **encoder layers**:\n",
    "  - they are totally **invariant to the position of the tokens**\n",
    "  \n",
    "  \n",
    "- Multi-head attention layer is effectively a weighted sum, the **information on token position is lost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8227fc8e",
   "metadata": {
    "papermill": {
     "duration": 0.022273,
     "end_time": "2022-12-28T18:25:05.849316",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.827043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## <b>6 <span style='color:#F1A424'>|</span> Positional Embeddings</b> \n",
    "\n",
    "***\n",
    "\n",
    "Incorporate positional information using `positional embeddings`:\n",
    "\n",
    "- `positional embeddings` are based on idea:\n",
    "  - Modify the `token embeddings` with a **position-dependent pattern** of values arranged in a vector\n",
    "  \n",
    "  \n",
    "- If the pattern is characteristic for each position, the attention heads and feed-forward layers in each stack can learn to incorporate positional information into their transformations\n",
    "- There are several ways to achieve this, and one of the most popular approaches is to use a `learnable pattern`\n",
    "- This works exactly the same way as the token embeddings, but using the **position index** instead of the **token identifier** (from vocabulary dictionary) as input\n",
    "- An efficient way of encoding the positions of tokens is learned during pretraining\n",
    "\n",
    "Creating Custom `Embedding` class\n",
    "\n",
    "Let’s create a custom Embeddings module (**token embeddings + positional embeddings**)\n",
    " - That combines a token embedding layer that projects the input_ids to a dense hidden state \n",
    " - Together with the positional embedding that does the same for position_ids\n",
    " - The resulting embedding is simply the **sum of both embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82541f0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:05.898062Z",
     "iopub.status.busy": "2022-12-28T18:25:05.897370Z",
     "iopub.status.idle": "2022-12-28T18:25:05.908522Z",
     "shell.execute_reply": "2022-12-28T18:25:05.905952Z"
    },
    "papermill": {
     "duration": 0.038683,
     "end_time": "2022-12-28T18:25:05.911836",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.873153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Token & Position Embedding Layer\n",
    "\n",
    "class tpEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):        \n",
    "        super().__init__()\n",
    "        \n",
    "        # token embedding layer\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size,\n",
    "                                             config.hidden_size)\n",
    "        \n",
    "        # positional embedding layer\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n",
    "                                                config.hidden_size)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        # Create position IDs for input sequence\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
    "        \n",
    "        # Create token and position embeddings\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        \n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ac4fb98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:05.960079Z",
     "iopub.status.busy": "2022-12-28T18:25:05.959653Z",
     "iopub.status.idle": "2022-12-28T18:25:06.121649Z",
     "shell.execute_reply": "2022-12-28T18:25:06.120301Z"
    },
    "papermill": {
     "duration": 0.190095,
     "end_time": "2022-12-28T18:25:06.125030",
     "exception": false,
     "start_time": "2022-12-28T18:25:05.934935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8694, -1.6714,  0.7159,  ...,  1.1703,  1.5813,  2.1954],\n",
       "         [-0.0000,  0.0672, -0.0000,  ...,  1.5110, -0.0000,  0.0000],\n",
       "         [ 1.1838,  0.3636,  0.0000,  ...,  3.7152,  1.0541, -0.0000],\n",
       "         [ 0.5582,  0.5616, -0.0000,  ..., -0.7990, -0.1277, -2.2744],\n",
       "         [-0.7716,  1.9271,  0.0000,  ...,  1.1052,  0.0000,  0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token and Position Embeddings\n",
    "embedding_layer = tpEmbedding(config)\n",
    "embedding_layer(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9d7a3",
   "metadata": {
    "papermill": {
     "duration": 0.021283,
     "end_time": "2022-12-28T18:25:06.170052",
     "exception": false,
     "start_time": "2022-12-28T18:25:06.148769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <b>7 <span style='color:#F1A424'>|</span> Putting it all Together</b> \n",
    "\n",
    "***\n",
    "\n",
    "Constructing the Transformer **<mark style=\"background-color:#FFC300;color:white;border-radius:5px;opacity:1.0\">encoder</mark>**, combining the `Embedding` and `Encoder`  layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0367774d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:06.218750Z",
     "iopub.status.busy": "2022-12-28T18:25:06.217164Z",
     "iopub.status.idle": "2022-12-28T18:25:06.225599Z",
     "shell.execute_reply": "2022-12-28T18:25:06.224464Z"
    },
    "papermill": {
     "duration": 0.035433,
     "end_time": "2022-12-28T18:25:06.228569",
     "exception": false,
     "start_time": "2022-12-28T18:25:06.193136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# full transformer encoder combining the `Embedding` with the ``Embedding` ` layers\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):       \n",
    "        super().__init__()\n",
    "        \n",
    "        # token & positional embedding layer\n",
    "        self.embeddings = tpEmbedding(config)\n",
    "        \n",
    "        # attention & forward feed layer \n",
    "        self.layers = nn.ModuleList([encoderLayer(config)\n",
    "                                     for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # embeddings\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc4b565e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:06.275091Z",
     "iopub.status.busy": "2022-12-28T18:25:06.274289Z",
     "iopub.status.idle": "2022-12-28T18:25:07.186957Z",
     "shell.execute_reply": "2022-12-28T18:25:07.185382Z"
    },
    "papermill": {
     "duration": 0.93896,
     "end_time": "2022-12-28T18:25:07.189971",
     "exception": false,
     "start_time": "2022-12-28T18:25:06.251011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.7086, -2.3515, -1.4191,  ...,  1.4979, -3.3497,  0.9201],\n",
       "         [ 2.0326,  1.8728, -5.9263,  ..., -1.3855,  0.0124,  2.9590],\n",
       "         [ 1.9051,  2.1667,  0.6003,  ..., -0.6429,  1.2111, -2.7430],\n",
       "         [ 1.5930, -0.1081, -2.5242,  ..., -2.8394, -0.5158,  0.1820],\n",
       "         [ 1.9846,  0.8312,  3.4625,  ..., -1.2064, -0.3385,  0.1955]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(config)\n",
    "encoder_output = encoder(inputs.input_ids)\n",
    "encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55959174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:07.239469Z",
     "iopub.status.busy": "2022-12-28T18:25:07.237532Z",
     "iopub.status.idle": "2022-12-28T18:25:07.247551Z",
     "shell.execute_reply": "2022-12-28T18:25:07.245546Z"
    },
    "papermill": {
     "duration": 0.03815,
     "end_time": "2022-12-28T18:25:07.250489",
     "exception": false,
     "start_time": "2022-12-28T18:25:07.212339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden state for each token in a batch\n",
    "encoder_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f039eb",
   "metadata": {
    "papermill": {
     "duration": 0.02125,
     "end_time": "2022-12-28T18:25:07.295377",
     "exception": false,
     "start_time": "2022-12-28T18:25:07.274127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <b>8 <span style='color:#F1A424'>|</span> Classification Head</b> \n",
    "\n",
    "***\n",
    "\n",
    "Quite often, transformers are divided into:\n",
    "- Task independent body (`TransformerEncoder`)\n",
    "- Task dependent head (`TransformerClassifier`)\n",
    "\n",
    "\n",
    "- We have a hidden state for each token (5 tokens x 768 dimensions) at the output, but need to make one prediction\n",
    "\n",
    "\n",
    "- The first token in such models is often used for the prediction **[CLS] token**\n",
    "  - can attach a dropout and a linear layer to make a classification prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f153a07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:07.340181Z",
     "iopub.status.busy": "2022-12-28T18:25:07.339429Z",
     "iopub.status.idle": "2022-12-28T18:25:07.347525Z",
     "shell.execute_reply": "2022-12-28T18:25:07.346381Z"
    },
    "papermill": {
     "duration": 0.033755,
     "end_time": "2022-12-28T18:25:07.350262",
     "exception": false,
     "start_time": "2022-12-28T18:25:07.316507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f04784",
   "metadata": {
    "papermill": {
     "duration": 0.021331,
     "end_time": "2022-12-28T18:25:07.393448",
     "exception": false,
     "start_time": "2022-12-28T18:25:07.372117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- For each sample in the batch we get the **unnormalized logits** for each class in the output, which corresponds to the BERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "236c3c5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-28T18:25:07.441780Z",
     "iopub.status.busy": "2022-12-28T18:25:07.440801Z",
     "iopub.status.idle": "2022-12-28T18:25:08.617443Z",
     "shell.execute_reply": "2022-12-28T18:25:08.615794Z"
    },
    "papermill": {
     "duration": 1.204065,
     "end_time": "2022-12-28T18:25:08.620479",
     "exception": false,
     "start_time": "2022-12-28T18:25:07.416414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5311,  1.9373, -0.6955]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_labels = 3\n",
    "encoder_classifier = TransformerClassifier(config)\n",
    "output = encoder_classifier(inputs.input_ids)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 71.451663,
   "end_time": "2022-12-28T18:25:11.412436",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-28T18:23:59.960773",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "017006a4d0564df8bedffa7634ca9ce5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a505593fe1824d59aacddd60a8b2b312",
       "placeholder": "​",
       "style": "IPY_MODEL_ca8e57e7442f430ca0aa656b5fb9d975",
       "value": " 455k/455k [00:00&lt;00:00, 893kB/s]"
      }
     },
     "03b6a385bf654eb495493eff4a275500": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7d03193f357043148c5132d4232172da",
       "max": 28.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5b7ce6e43d3d4ef3a3bd36d0f4fc3459",
       "value": 28.0
      }
     },
     "057b195ca1aa4f95a623afcc177dc5db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "08383d0cffd340d18dbcfee7ac8d7f83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1482aa9fb28d4262b06c4180b1584ab8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6a52369d2b3d4dc798b79058a53c3424",
       "placeholder": "​",
       "style": "IPY_MODEL_83fb648ba0ad4ac38c37b8648db59071",
       "value": " 570/570 [00:00&lt;00:00, 17.5kB/s]"
      }
     },
     "2336574aff084635add4e63e4a6bb684": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2401a6c3392b4e118fd2a9ff987c10af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "32742b244a6b490e953397e1d756a32b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_057b195ca1aa4f95a623afcc177dc5db",
       "placeholder": "​",
       "style": "IPY_MODEL_08383d0cffd340d18dbcfee7ac8d7f83",
       "value": "Downloading: 100%"
      }
     },
     "351f999b80a24353a44210b7bfc8dd2c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_70c551673da945ccad1eceb01858aef3",
       "placeholder": "​",
       "style": "IPY_MODEL_e4515ad55d2b4dcdb2402e7f99dc5d7d",
       "value": "Downloading: 100%"
      }
     },
     "3cf2d0efbc654a6292c3a833b93e728e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_351f999b80a24353a44210b7bfc8dd2c",
        "IPY_MODEL_a4907018476047c488adee5942a81965",
        "IPY_MODEL_017006a4d0564df8bedffa7634ca9ce5"
       ],
       "layout": "IPY_MODEL_78ea8d71cc2947fba70488193b645f4c"
      }
     },
     "4073cd6f5976482d84a836ba30dfde7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_90ff0138cc664bc1a29f44b376e31330",
        "IPY_MODEL_03b6a385bf654eb495493eff4a275500",
        "IPY_MODEL_daff34c14f8a4e4e820bbb3086ba7eaf"
       ],
       "layout": "IPY_MODEL_785fcd53b70f4488b94ed358d0e2f1d4"
      }
     },
     "4455f4779f3448a3b128ac1c3b06dcdf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4d3bfd48e05e4b8daddbcba7203a65a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4dd23594580849b59b1f938cb7d3db9e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4f27b79eabdb482ba253e15bb6c6f85a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b7ce6e43d3d4ef3a3bd36d0f4fc3459": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6a52369d2b3d4dc798b79058a53c3424": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6bb591f1e7464dbbac9f8dda77615d03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4dd23594580849b59b1f938cb7d3db9e",
       "max": 231508.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7a570672f015466fbb67c576676072c9",
       "value": 231508.0
      }
     },
     "6e8fc3e96bf24ef48dc45c86daac0ec2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "70c551673da945ccad1eceb01858aef3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "785fcd53b70f4488b94ed358d0e2f1d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "78ea8d71cc2947fba70488193b645f4c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a570672f015466fbb67c576676072c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7d03193f357043148c5132d4232172da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "83fb648ba0ad4ac38c37b8648db59071": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8e557bbfea9c400e8d4c72a791ed11c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9c1ac1b8d4144b3ca099863ded9803ba",
       "placeholder": "​",
       "style": "IPY_MODEL_fe079a5ebfd84957884264da64dc1ce4",
       "value": "Downloading: 100%"
      }
     },
     "90084adecf2149d481cad8cefc648ada": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6e8fc3e96bf24ef48dc45c86daac0ec2",
       "placeholder": "​",
       "style": "IPY_MODEL_c3dce96663994914aca0bedb069f8bd6",
       "value": " 226k/226k [00:00&lt;00:00, 670kB/s]"
      }
     },
     "90ff0138cc664bc1a29f44b376e31330": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4455f4779f3448a3b128ac1c3b06dcdf",
       "placeholder": "​",
       "style": "IPY_MODEL_b3d76d37751b4a9284b26597ef660e98",
       "value": "Downloading: 100%"
      }
     },
     "9321701727424b899f550220dab1793a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8e557bbfea9c400e8d4c72a791ed11c9",
        "IPY_MODEL_6bb591f1e7464dbbac9f8dda77615d03",
        "IPY_MODEL_90084adecf2149d481cad8cefc648ada"
       ],
       "layout": "IPY_MODEL_4d3bfd48e05e4b8daddbcba7203a65a9"
      }
     },
     "9c1ac1b8d4144b3ca099863ded9803ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a30012a7ec86477bb4fa6350e261e122": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a48959fb65aa40989b6a58dc4eee4faa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a30012a7ec86477bb4fa6350e261e122",
       "max": 570.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ae450f364fd34721a40ff5ae225d167e",
       "value": 570.0
      }
     },
     "a4907018476047c488adee5942a81965": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c8477d20ab7742d1b2d55afaee739282",
       "max": 466062.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2336574aff084635add4e63e4a6bb684",
       "value": 466062.0
      }
     },
     "a505593fe1824d59aacddd60a8b2b312": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ae450f364fd34721a40ff5ae225d167e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b3d76d37751b4a9284b26597ef660e98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c3dce96663994914aca0bedb069f8bd6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c8477d20ab7742d1b2d55afaee739282": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ca8e57e7442f430ca0aa656b5fb9d975": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "daff34c14f8a4e4e820bbb3086ba7eaf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4f27b79eabdb482ba253e15bb6c6f85a",
       "placeholder": "​",
       "style": "IPY_MODEL_ed29bff2f7804418a6d5111e7bbde9cd",
       "value": " 28.0/28.0 [00:00&lt;00:00, 871B/s]"
      }
     },
     "e4515ad55d2b4dcdb2402e7f99dc5d7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ed29bff2f7804418a6d5111e7bbde9cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ed44b1c23dde4812b5b601a763c51fa1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_32742b244a6b490e953397e1d756a32b",
        "IPY_MODEL_a48959fb65aa40989b6a58dc4eee4faa",
        "IPY_MODEL_1482aa9fb28d4262b06c4180b1584ab8"
       ],
       "layout": "IPY_MODEL_2401a6c3392b4e118fd2a9ff987c10af"
      }
     },
     "fe079a5ebfd84957884264da64dc1ce4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
